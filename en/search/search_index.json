{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Data Transfer Hub solution provides secure, scalable, and trackable data transfer for Amazon Simple Storage Service (Amazon S3) objects and Amazon Elastic Container Registry (Amazon ECR) images. This data transfer helps customers expand their businesses globally in and out of AWS China Regions. The solution\u2019s web console provides an interface for managing the following tasks: Transferring Amazon S3 objects between AWS China Regions and AWS Standard Regions Transferring data from other cloud providers\u2019 object storage services (including Alibaba Cloud OSS, Tencent COS, and Qiniu Kodo) to Amazon S3 Transferring objects from Amazon S3 compatible object storage service to Amazon S3 Transferring Amazon ECR images between AWS China Regions and AWS Standard Regions Transferring container images from public container registries (for example, Docker Hub, Google gcr.io, Red Hat Quay.io) to Amazon ECR Note If you need to transfer Amazon S3 objects between AWS Standard Regions, we recommend that you use Cross-Region Replication ; if you want to transfer Amazon S3 objects within the same AWS Standard Region, we recommend using Same-Region Replication . This implementation guide describes architectural considerations and configuration steps for deploying Data Transfer Hub in the Amazon Web Services (AWS) Cloud. It includes links to an AWS CloudFormation template that launches and configures the AWS services required to deploy this solution using AWS best practices for security and availability. The guide is intended for IT architects, developers, DevOps, data analysts, and marketing technology professionals who have practical experience architecting in the AWS Cloud.","title":"Welcome"},{"location":"additional-resources/","text":"AWS CloudFormation Amazon S3 AWS Lambda AWS Step Functions Amazon CloudFront Amazon ECR Amazon DynamoDB AWS AppSync Amazon Cognito AWS IAM Amazon EC2 Amazon Route 53","title":"Additional resources"},{"location":"architecture/","text":"Deploying the Data Transfer Hub solution with the default parameters builds the following environment in the AWS Cloud. Figure 1: Data Transfer Hub architecture The solution automatically deploys and configures a serverless architecture with the following services: The solution\u2019s static web assets (frontend user interface) are stored in Amazon S3 and made available through Amazon CloudFront . The backend APIs are provided via AWS AppSync GraphQL. Users are authenticated by either Amazon Cognito User Pool (in AWS Standard Regions) or by an OpenID connect provider (in AWS China Regions) such as Authing , Auth0 , etc. AWS AppSync runs AWS Lambda to call backend APIs. Lambda starts an AWS Step Functions workflow that uses AWS CloudFormation to start or stop/delete the ECR or S3 plugin template. The plugin templates are hosted in a centralized Amazon S3 bucket manged by AWS. The solution also provisions an Amazon ECS cluster that runs the container images used by the plugin template, and the container images are hosted in Amazon ECR . The data transfer task information is stored in in Amazon DynamoDB . Important If you deploy this solution in AWS (Beijing) Region operated by Beijing Sinnet Technology Co., Ltd. (Sinnet), or the AWS (Ningxia) Region operated by Ningxia Western Cloud Data Technology Co., Ltd. ( ), you are required to provide a domain with ICP Recordal before you can access the web console. The web console is a centralized place to create and manage all data transfer jobs. Each data type (for example, Amazon S3 or Amazon ECR) is a plugin for Data Transfer Hub, and is packaged as an AWS CloudFormation template hosted in an S3 bucket that AWS owns. When the you create a transfer task, an AWS Lambda function initiates the Amazon CloudFormation template, and state of each task is stored and displayed in the DynamoDB tables. As of December 2022, the solution supports two data transfer plugins: an Amazon S3 plugin and an Amazon ECR plugin. Amazon S3 plugin Figure 2: Data Transfer Hub Amazon S3 plugin architecture The Amazon S3 plugin runs the following workflows: A time-based Event Bridge rule triggers a AWS Lambda function on an hourly basis. AWS Lambda uses the launch template to launch a data comparison job (JobFinder) in an EC2. The job lists all the objects in the source and destination buckets, makes comparisons among objects and determines which objects should be transferred. EC2 sends a message for each object that will be transferred to Amazon Simple Queue Service (Amazon SQS). Amazon S3 event messages can also be supported for more real-time data transfer; whenever there is object uploaded to source bucket, the event message is sent to the same SQS queue. A JobWorker running in EC2 consumes the messages in SQS and transfers the object from the source bucket to the destination bucket. You can use an Auto Scaling Group to control the number of EC2 instances to transfer the data based on business need. A record with transfer status for each object is stored in Amazon DynamoDB. The Amazon EC2 instance will get (download) the object from the source bucket based on the SQS message. The EC2 instance will put (upload) the object to the destination bucket based on the SQS message. Note If an object (or part of an object) failed to transfer, the JobWorker releases the message in the queue, and the object is transferred again after the message is visible in the queue (default visibility timeout is set to 15 minutes). If the transfer fails again, the message is sent to the dead letter queue and a notification alarm is sent. Amazon ECR plugin Figure 3: Data Transfer Hub Amazon ECR plugin architecture The Amazon ECR plugin runs the following workflows: An EventBridge rule runs an AWS Step Functions workflow on a regular basis (by default, it runs daily). Step Functions invokes AWS Lambda to retrieve the list of images from the source. Lambda will either list all the repository content in the source Amazon ECR, or get the stored image list from System Manager Parameter Store. The transfer task will run within Fargate in a maximum concurrency of 10. If a transfer task failed for some reason, it will automatically retry three times. Each task uses skopeo to copy the images into the target ECR. After the copy completes, the status (either success or fail) is logged into DynamoDB for tracking purpose.","title":"Architecture overview"},{"location":"architecture/#amazon-s3-plugin","text":"Figure 2: Data Transfer Hub Amazon S3 plugin architecture The Amazon S3 plugin runs the following workflows: A time-based Event Bridge rule triggers a AWS Lambda function on an hourly basis. AWS Lambda uses the launch template to launch a data comparison job (JobFinder) in an EC2. The job lists all the objects in the source and destination buckets, makes comparisons among objects and determines which objects should be transferred. EC2 sends a message for each object that will be transferred to Amazon Simple Queue Service (Amazon SQS). Amazon S3 event messages can also be supported for more real-time data transfer; whenever there is object uploaded to source bucket, the event message is sent to the same SQS queue. A JobWorker running in EC2 consumes the messages in SQS and transfers the object from the source bucket to the destination bucket. You can use an Auto Scaling Group to control the number of EC2 instances to transfer the data based on business need. A record with transfer status for each object is stored in Amazon DynamoDB. The Amazon EC2 instance will get (download) the object from the source bucket based on the SQS message. The EC2 instance will put (upload) the object to the destination bucket based on the SQS message. Note If an object (or part of an object) failed to transfer, the JobWorker releases the message in the queue, and the object is transferred again after the message is visible in the queue (default visibility timeout is set to 15 minutes). If the transfer fails again, the message is sent to the dead letter queue and a notification alarm is sent.","title":"Amazon S3 plugin"},{"location":"architecture/#amazon-ecr-plugin","text":"Figure 3: Data Transfer Hub Amazon ECR plugin architecture The Amazon ECR plugin runs the following workflows: An EventBridge rule runs an AWS Step Functions workflow on a regular basis (by default, it runs daily). Step Functions invokes AWS Lambda to retrieve the list of images from the source. Lambda will either list all the repository content in the source Amazon ECR, or get the stored image list from System Manager Parameter Store. The transfer task will run within Fargate in a maximum concurrency of 10. If a transfer task failed for some reason, it will automatically retry three times. Each task uses skopeo to copy the images into the target ECR. After the copy completes, the status (either success or fail) is logged into DynamoDB for tracking purpose.","title":"Amazon ECR plugin"},{"location":"cost/","text":"You are responsible for the cost of the AWS services used while running this solution, which can vary based on whether you are transferring Amazon S3 objects or Amazon ECR images. The solution automatically deploys an additional Amazon CloudFront Distribution and an Amazon S3 bucket for storing the static website assets in your account. You are responsible for the incurred variable charges from these services. For full details, refer to the pricing webpage for each AWS service you will be using in this solution. The following three examples demonstrate how to estimate the cost. Two example estimates are for transferring S3 objects, and one is for transferring ECR images. Cost of an Amazon S3 transfer task For an Amazon S3 transfer task, the cost can vary based on the total number of files and the average file size. Example 1 Transfer 1 TB of S3 files from AWS Oregon Region (us-west-2) to AWS Beijing Region (cn-north-1), the average file size is 50MB . Total files: ~20,480 Average speed per EC2 instance: ~1GB/min Total EC2 instance hours: ~17 hours As of December 2022, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost Amazon EC2 $0.0084 per hour (t4g.micro) $0.14 Amazon S3 ~ 12 GET requests + 10 PUT requests per file GET: $0.0004 per 1000 request PUT: $0.005 per 1000 request $1.12 Amazon DynamoDB ~2 write requests per file $1.25 per million write $0.05 Amazon SQS ~2 request per file $0.40 per million request $0.01 Data Transfer Out 0.09 per GB $92.16 Others (For example, CloudWatch, Secrets Manager, etc.) $1 TOTAL $94.48 Example 2 Transfer 1 TB of S3 files from AWS Oregon region (us-west-2) to China Beijing Region (cn-north-1), the average file size is 10KB . Total files: ~107,400,000 Average speed per EC2 instance: ~6MB/min (~10 files per sec) Total EC2 instance hours: ~3000 hours As of December 2022, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost Amazon EC2 $0.0084 per hour (t4g.micro) $25.20 Amazon S3 ~ 2 GET requests + 1 PUT request per file GET: $0.0004 per 1000 request PUT: $0.005 per 1000 request $622.34 Amazon DynamoDB ~2 write requests per file $1.25 per million write $268.25 Amazon SQS ~2 requests per file $0.40 per million request $85.92 Data Transfer Out 0.09 per GB $92.16 Others (For example, CloudWatch, Secrets Manager, etc.) $20 TOTAL $1113.87 Cost of an Amazon ECR transfer task For an Amazon ECR transfer task, the cost can vary based on network speed and total size of ECR images. Example 3 Transfer 27 Amazon ECR images (~3 GB in total size) from AWS Ireland Region (eu-west-1) to AWS Beijing Region (cn-north-1). The total runtime is about 6 minutes. As of December 2022, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost AWS Lambda $0.0000004 per 100ms $0.000072 (35221.95 ms) AWS Step Functions $0.000025 per State Transition (~ 60 state transitions per run in this case) $0.0015 Fargate $0.04048 per vCPU per hour $0.004445 per GB per hour (0.5 vCPU 1GB Memory) $0.015 (~ 2200s) Data Transfer Out 0.09 per GB $0.27 Others (For example, CloudWatch, Secrets Manager, etc.) $0 TOTAL $0.287","title":"Cost"},{"location":"cost/#cost-of-an-amazon-s3-transfer-task","text":"For an Amazon S3 transfer task, the cost can vary based on the total number of files and the average file size.","title":"Cost of an Amazon S3 transfer task"},{"location":"cost/#example-1","text":"Transfer 1 TB of S3 files from AWS Oregon Region (us-west-2) to AWS Beijing Region (cn-north-1), the average file size is 50MB . Total files: ~20,480 Average speed per EC2 instance: ~1GB/min Total EC2 instance hours: ~17 hours As of December 2022, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost Amazon EC2 $0.0084 per hour (t4g.micro) $0.14 Amazon S3 ~ 12 GET requests + 10 PUT requests per file GET: $0.0004 per 1000 request PUT: $0.005 per 1000 request $1.12 Amazon DynamoDB ~2 write requests per file $1.25 per million write $0.05 Amazon SQS ~2 request per file $0.40 per million request $0.01 Data Transfer Out 0.09 per GB $92.16 Others (For example, CloudWatch, Secrets Manager, etc.) $1 TOTAL $94.48","title":"Example 1"},{"location":"cost/#example-2","text":"Transfer 1 TB of S3 files from AWS Oregon region (us-west-2) to China Beijing Region (cn-north-1), the average file size is 10KB . Total files: ~107,400,000 Average speed per EC2 instance: ~6MB/min (~10 files per sec) Total EC2 instance hours: ~3000 hours As of December 2022, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost Amazon EC2 $0.0084 per hour (t4g.micro) $25.20 Amazon S3 ~ 2 GET requests + 1 PUT request per file GET: $0.0004 per 1000 request PUT: $0.005 per 1000 request $622.34 Amazon DynamoDB ~2 write requests per file $1.25 per million write $268.25 Amazon SQS ~2 requests per file $0.40 per million request $85.92 Data Transfer Out 0.09 per GB $92.16 Others (For example, CloudWatch, Secrets Manager, etc.) $20 TOTAL $1113.87","title":"Example 2"},{"location":"cost/#cost-of-an-amazon-ecr-transfer-task","text":"For an Amazon ECR transfer task, the cost can vary based on network speed and total size of ECR images.","title":"Cost of an Amazon ECR transfer task"},{"location":"cost/#example-3","text":"Transfer 27 Amazon ECR images (~3 GB in total size) from AWS Ireland Region (eu-west-1) to AWS Beijing Region (cn-north-1). The total runtime is about 6 minutes. As of December 2022, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost AWS Lambda $0.0000004 per 100ms $0.000072 (35221.95 ms) AWS Step Functions $0.000025 per State Transition (~ 60 state transitions per run in this case) $0.0015 Fargate $0.04048 per vCPU per hour $0.004445 per GB per hour (0.5 vCPU 1GB Memory) $0.015 (~ 2200s) Data Transfer Out 0.09 per GB $0.27 Others (For example, CloudWatch, Secrets Manager, etc.) $0 TOTAL $0.287","title":"Example 3"},{"location":"deployment/","text":"Before you launch the solution, review the cost, architecture, network security, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account. Time to deploy : Approximately 15 minutes Deployment overview Use the following steps to deploy this solution on AWS. For detailed instructions, follow the links for each step. Step 1. Launch the stack (Option 1) Deploy the AWS CloudFormation template in AWS Standard Regions (Option 2) Deploy the AWS CloudFormation template in AWS China Regions Step 2. Launch the web console Step 3. Create a transfer task Step 1. (Option 1) Launch the stack in AWS Standard Regions Important The following deployment instructions apply to AWS Standard Regions only. For deployment in AWS China Regions, refer to Option 2. Deploy the AWS CloudFormation template for Option 1 \u2013 AWS Standard Regions Note You are responsible for the cost of the AWS services used while running this solution. For more details, visit the Cost section in this guide, and refer to the pricing webpage for each AWS service used in this solution. Sign in to the AWS Management Console and select the button to launch the DataTransferHub-cognito.template AWS CloudFormation template. Alternatively, you can download the template as a starting point for your own implementation. The template launches in the US East (N. Virginia) Region by default. To launch the solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for this solution template and modify them as necessary. This solution uses the following default values. Parameter Default Description AdminEmail <Requires input> The email of the Admin user. Choose Next . On the Configure Stack Options page, keep the default values and choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. Step 1. (Option 2) Launch the stack in AWS China Regions Important The following deployment instructions apply to AWS China Regions only. For deployment in AWS Standard Regions, refer to Option 1. Prerequisites Create an OIDC user pool. Configure domain name service (DNS) resolution. Make sure a domain registered by ICP is available. Prerequisite 1: Create an OIDC user pool In AWS Regions where Amazon Cognito is not yet available, you can use OIDC to provide authentication. The following procedure uses AWS Partner Authing as an example, but you can also choose any available provider. Go to the Authing console . Create a new user pool if you don't have one. Select the user pool. On the left navigation bar, select Self-built App under Applications . Click the Create button. Enter the Application Name , and Subdomain . Save the App ID (that is, client_id ) and Issuer to a text file from Endpoint Information, which will be used later. Update the Login Callback URL and Logout Callback URL to your IPC recorded domain name. Set the Authorization Configuration. Update login control. Select and enter the Application interface from the left sidebar, select Login Control , and then select Registration and Login . Please select only Password Login: Email for the login method. Please uncheck all options in the registration method. Select Save . Create an admin user. From Users & Roles , select Users , then choose Create user . Enter the email for the user. Choose OK . Check the email for a temporary password. Reset the user password. Note Because this solution does not support application roles, all the users will receive admin rights. Prerequisite 2: Configure domain name service resolution Configure domain name service (DNS) resolution to point the ICP licensed domain to the CloudFront default domain name. Optionally, you can use your own DNS resolver. The following is an example for configuring an Amazon Route 53. Create a hosted zone in Amazon Route 53. For more information, refer to the Amazon Route 53 Developer Guide . Create a CNAME record for the console URL. From the hosted zone, choose Create Record . In the Record name input box, enter the host name. From Record type select CNAME . In the value field, Enter the CloudFormation output PortalUrl. Select Create records . Add alternative domain names to the CloudFront distribution. Configure the corresponding domain name in CloudFront to open the CloudFront console by finding the distribution ID for PortalURL in the list and selecting ID (or check the check box, and then select Distribution Settings ). Click Edit , and add the record of Route 53 in the previous step to the Alternate Domain Name (CNAME) . Deploy the AWS CloudFormation template for Option 2 \u2013 AWS China Regions This automated AWS CloudFormation template deploys Data Transfer Hub in the AWS Cloud. You must Create an ODIC User Pool and Configure DNS resolution before launching the stack. Note You are responsible for the cost of the AWS services used while running this solution. For more details, visit the Cost section in this guide, and refer to the pricing webpage for each AWS service used in this solution. Sign in to the AWS Management Console and select the button to launch the DataTransferHub-openid.template AWS CloudFormation template. Alternatively, you can download the template as a starting point for your own implementation. The template launches in your console\u2019s default Region. To launch the solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for this solution template and modify them as necessary. This solution uses the following default values. Parameter Default Description OidcProvider <Requires input> Refers to the Issuer shown in the OIDC application configuration. OidcClientId <Requires input> Refers to the App ID shown in the OIDC application configuration. OidcCustomerDomain <Requires input> Refers to the customer domain that has completed ICP registration in China, not the subdomain provided by Authing. It must start with https:// . AdminEmail <Requires input> Refers to the email for receiving task status alarm. Choose Next . On the Configure Stack Options page, keep the default values and choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create Stack to deploy the stack. You can view the status of your stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. Step 2. Launch the web console After the stack is successfully created, navigate to the CloudFormation Outputs tab and select the PortalUrl value to access the Data Transfer Hub web console. After successful deployment, an email containing the temporary login password will be sent to the email address provided. Depending on the region where you start the stack, you can choose to access the web console from the AWS China Regions or the AWS Standard Regions. Log in with Amazon Cognito User Pool (for AWS Standard Regions) Log in with OpenID using Authing.cn (for AWS China Regions) (Option 1) Log in using Amazon Cognito user pool for AWS Standard Regions Using a web browser, enter the PortalURL from the CloudFormation Output tab, then navigate to the Amazon Cognito console. Sign in with the AdminEmail and the temporary password. Set a new account password. (Optional) Verify your email address for account recovery. After the verification is complete, the system opens the Data Transfer Hub web console. (Option 2) OpenID authentication for AWS China Regions Using a web browser, enter the Data Transfer Hub domain name. If you are logging in for the first time, the system will open the Authing.cn login interface. Enter the username and password you registered when you deployed the solution, then choose Login. The system opens the Data Transfer Hub web console. Change your password and then sign in again. Step 3. Create a transfer task Use the web console to create a transfer task for Amazon S3 or Amazon ECR. For more information, refer to Create Amazon S3 Transfer Task and Create Amazon ECR Transfer Task . Figure 1: Data Transfer Hub web console","title":"Automated deployment"},{"location":"deployment/#deployment-overview","text":"Use the following steps to deploy this solution on AWS. For detailed instructions, follow the links for each step. Step 1. Launch the stack (Option 1) Deploy the AWS CloudFormation template in AWS Standard Regions (Option 2) Deploy the AWS CloudFormation template in AWS China Regions Step 2. Launch the web console Step 3. Create a transfer task","title":"Deployment overview"},{"location":"deployment/#step-1-option-1-launch-the-stack-in-aws-standard-regions","text":"Important The following deployment instructions apply to AWS Standard Regions only. For deployment in AWS China Regions, refer to Option 2. Deploy the AWS CloudFormation template for Option 1 \u2013 AWS Standard Regions Note You are responsible for the cost of the AWS services used while running this solution. For more details, visit the Cost section in this guide, and refer to the pricing webpage for each AWS service used in this solution. Sign in to the AWS Management Console and select the button to launch the DataTransferHub-cognito.template AWS CloudFormation template. Alternatively, you can download the template as a starting point for your own implementation. The template launches in the US East (N. Virginia) Region by default. To launch the solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for this solution template and modify them as necessary. This solution uses the following default values. Parameter Default Description AdminEmail <Requires input> The email of the Admin user. Choose Next . On the Configure Stack Options page, keep the default values and choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Step 1. (Option 1) Launch the stack in AWS Standard Regions "},{"location":"deployment/#step-1-option-2-launch-the-stack-in-aws-china-regions","text":"Important The following deployment instructions apply to AWS China Regions only. For deployment in AWS Standard Regions, refer to Option 1.","title":"Step 1. (Option 2) Launch the stack in AWS China Regions "},{"location":"deployment/#prerequisites","text":"Create an OIDC user pool. Configure domain name service (DNS) resolution. Make sure a domain registered by ICP is available.","title":"Prerequisites"},{"location":"deployment/#prerequisite-1-create-an-oidc-user-pool","text":"In AWS Regions where Amazon Cognito is not yet available, you can use OIDC to provide authentication. The following procedure uses AWS Partner Authing as an example, but you can also choose any available provider. Go to the Authing console . Create a new user pool if you don't have one. Select the user pool. On the left navigation bar, select Self-built App under Applications . Click the Create button. Enter the Application Name , and Subdomain . Save the App ID (that is, client_id ) and Issuer to a text file from Endpoint Information, which will be used later. Update the Login Callback URL and Logout Callback URL to your IPC recorded domain name. Set the Authorization Configuration. Update login control. Select and enter the Application interface from the left sidebar, select Login Control , and then select Registration and Login . Please select only Password Login: Email for the login method. Please uncheck all options in the registration method. Select Save . Create an admin user. From Users & Roles , select Users , then choose Create user . Enter the email for the user. Choose OK . Check the email for a temporary password. Reset the user password. Note Because this solution does not support application roles, all the users will receive admin rights.","title":"Prerequisite 1: Create an OIDC user pool"},{"location":"deployment/#prerequisite-2-configure-domain-name-service-resolution","text":"Configure domain name service (DNS) resolution to point the ICP licensed domain to the CloudFront default domain name. Optionally, you can use your own DNS resolver. The following is an example for configuring an Amazon Route 53. Create a hosted zone in Amazon Route 53. For more information, refer to the Amazon Route 53 Developer Guide . Create a CNAME record for the console URL. From the hosted zone, choose Create Record . In the Record name input box, enter the host name. From Record type select CNAME . In the value field, Enter the CloudFormation output PortalUrl. Select Create records . Add alternative domain names to the CloudFront distribution. Configure the corresponding domain name in CloudFront to open the CloudFront console by finding the distribution ID for PortalURL in the list and selecting ID (or check the check box, and then select Distribution Settings ). Click Edit , and add the record of Route 53 in the previous step to the Alternate Domain Name (CNAME) . Deploy the AWS CloudFormation template for Option 2 \u2013 AWS China Regions This automated AWS CloudFormation template deploys Data Transfer Hub in the AWS Cloud. You must Create an ODIC User Pool and Configure DNS resolution before launching the stack. Note You are responsible for the cost of the AWS services used while running this solution. For more details, visit the Cost section in this guide, and refer to the pricing webpage for each AWS service used in this solution. Sign in to the AWS Management Console and select the button to launch the DataTransferHub-openid.template AWS CloudFormation template. Alternatively, you can download the template as a starting point for your own implementation. The template launches in your console\u2019s default Region. To launch the solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and STS Limits in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for this solution template and modify them as necessary. This solution uses the following default values. Parameter Default Description OidcProvider <Requires input> Refers to the Issuer shown in the OIDC application configuration. OidcClientId <Requires input> Refers to the App ID shown in the OIDC application configuration. OidcCustomerDomain <Requires input> Refers to the customer domain that has completed ICP registration in China, not the subdomain provided by Authing. It must start with https:// . AdminEmail <Requires input> Refers to the email for receiving task status alarm. Choose Next . On the Configure Stack Options page, keep the default values and choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create Stack to deploy the stack. You can view the status of your stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Prerequisite 2: Configure domain name service resolution"},{"location":"deployment/#step-2-launch-the-web-console","text":"After the stack is successfully created, navigate to the CloudFormation Outputs tab and select the PortalUrl value to access the Data Transfer Hub web console. After successful deployment, an email containing the temporary login password will be sent to the email address provided. Depending on the region where you start the stack, you can choose to access the web console from the AWS China Regions or the AWS Standard Regions. Log in with Amazon Cognito User Pool (for AWS Standard Regions) Log in with OpenID using Authing.cn (for AWS China Regions)","title":"Step 2. Launch the web console "},{"location":"deployment/#option-1-log-in-using-amazon-cognito-user-pool-for-aws-standard-regions","text":"Using a web browser, enter the PortalURL from the CloudFormation Output tab, then navigate to the Amazon Cognito console. Sign in with the AdminEmail and the temporary password. Set a new account password. (Optional) Verify your email address for account recovery. After the verification is complete, the system opens the Data Transfer Hub web console.","title":"(Option 1) Log in using Amazon Cognito user pool for AWS Standard Regions "},{"location":"deployment/#option-2-openid-authentication-for-aws-china-regions","text":"Using a web browser, enter the Data Transfer Hub domain name. If you are logging in for the first time, the system will open the Authing.cn login interface. Enter the username and password you registered when you deployed the solution, then choose Login. The system opens the Data Transfer Hub web console. Change your password and then sign in again.","title":"(Option 2) OpenID authentication for AWS China Regions "},{"location":"deployment/#step-3-create-a-transfer-task","text":"Use the web console to create a transfer task for Amazon S3 or Amazon ECR. For more information, refer to Create Amazon S3 Transfer Task and Create Amazon ECR Transfer Task . Figure 1: Data Transfer Hub web console","title":"Step 3. Create a transfer task "},{"location":"faq/","text":"The following are common issues you may face in deploying and using the solution. Deployment 1. In which AWS Regions can this solution be deployed? For the list of supported regions, refer to supported regions . 2. When creating a transfer task, shall I deploy it on the data source side or the destination side? The transfer performance of the solution will not be affected by whether the deployment is on the data source or destination side. If you do not have a domain name registered by ICP in AWS China Regions, we recommend you deploy it in AWS Standard Regions. If you need to deploy in AWS China Regions but do not have a domain name, you can directly deploy the back-end version: Amazon S3 Plugin: https://github.com/awslabs/amazon-s3-data-replication-hub-plugin Amazon ECR Plugin: https://github.com/awslabs/amazon-ecr-data-replication-hub-plugin 3. Do I need to deploy the solution on the data source and destination side separately? No. You can choose to deploy on the data source or destination side, which has no impact on the transfer performance. 4. Is it possible to deploy the solution in AWS account A and transfer Amazon S3 objects from account B to account C? Yes. In this case, you need to store the AccessKeyID and SecretAccessKey of account B and account C in the Secrets Manager of account A. 5. For data transfer within the production account, is it recommended to create an AWS account specifically for deploying the solution? Yes. It is recommended to create a new AWS account dedicated to deploying solutions. The account-level isolation improves the stability of the production account in the data synchronization process. 6. Is it possible to transfer data between different areas under the same account? Not supported currently. For this scenario, we recommend using Amazon S3's Cross-Region Replication . 7. Can I use AWS CLI to create a DTH S3 Transfer Task? Yes. Please refer to the tutorial Using AWS CLI to launch DTH S3 Transfer task . Performance 1. Will there be any difference in data transfer performance for deployment in AWS China Regions and in AWS Standard Regions? No. If you do not have a domain name registered by ICP in AWS China Regions, it is recommended to deploy it in the AWS Standard Regions. 2. What are the factors influencing the data transfer performance? The transfer performance may be affected by average file size, destination of data transfer, geographic location of data source, and real-time network environment. For example, using the same configuration, the transfer speed with an average file size of 50MB is 170 times the transfer speed with an average file size of 10KB. 3. What is the scale up/scale down policy of Worker Auto Scaling Group? The Auto Scaling Group will automatically scale up or scale down according to the number of tasks in SQS. Scaling Up Steps are: { lower : 100 , cha n ge : + 1 } { lower : 500 , cha n ge : + 2 } { lower : 2000 , cha n ge : + 5 } { lower : 10000 , cha n ge : + 10 } Scaling Down Step is: { upper : 0 , cha n ge : -10000 } Data security and authentication 1. How does the solution ensure data security? The solution adopts the following to ensure data security: All data is transferred in the memory in the transfer node cluster, without being placed on the disk. The external ports of all transfer nodes are closed, and there is no way to SSH into the transfer node. All data download and upload bottom layers are calling AWS official API, and data transfer conforms to the TLS protocol . 2. How does the solution ensure the security of resources on the cloud? In the research and development process, we strictly follow the minimum IAM permission design rules, and adopt the design of Auto Scaling, which will automatically help users terminate idle working nodes. 3. Is the front-end console open to the public network? How to ensure user authentication and multi-user management? Yes. You can access it with a front-end console link. User authentication and multi-user management are achieved through AWS Cognito User Pool in AWS Standard Regions, and through OIDC SAAS in AWS China Regions. 4. How does the solution achieve cross-account and cross-cloud authentication? By authentication through the Access Keyid and Access Key of the other party\u2019s account. The secret key is stored in AWS Secrets Manager and will be read in Secrets Manager as needed. 5. Does the solution support SSE-S3, SSE-KMS, and SSE-CMK? Yes. The solution supports the use of SSE-S3 and SSE-KMS data sources. If your source bucket has SSE-CMK enabled, refer to the tutorial . Features 1. What third-party clouds does Amazon S3 sync currently support? Alibaba Cloud OSS, Tencent Cloud, Huawei Cloud, Qiniu Cloud, Baidu Cloud, and all clouds that support S3 compatible protocols. 2. Why is the status of Task still in progress after all destination files are transferred? When will the task stop? For Fixed Rate Job The data difference between the data source and destination will be monitored continuously, and the differences between the two sides will be automatically compared after the first deployment. Moreover, when the default comparison task once an hour finds a difference, it will also transfer the difference data. Therefore, the status of the Task will always be in progress, unless the user manually terminates the task. Based on the built-in automatic expansion function of the solution, when there is no data to be transferred, the number of transfer working nodes will be automatically reduced to the minimum value configured by the user. For One Time Transfer Job When the objects are all transferred to the destination, the status of one time transfer job will become Completed . The transfer action will stop and you can select Stop to delete and release all backend resources. 3. How often will the data difference between the data source and destination be compared\uff1f By default, it runs hourly. At Task Scheduling Settings , you can make the task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . 4. Is it possible for real-time synchronization of newly added files? Near-real-time synchronization can be achieved, only if the Data Transfer Hub is deployed in the same AWS account and the same region as the data source. If the data source and the solution are not in the same account, you can configure it manually. For more information, refer to the tutorial . 5. Are there restrictions on the number of files and the size of files? No. Larger files will be uploaded in chunks. 6. If a single file transfer fails due to network issues, how to resolve it? Is there an error handling mechanism? There will be 5 retries. After 5 retries without success, the task will be notified to the user via email. 7. How to monitor the progress of the transfer by checking information like how many files are waiting to be transferred and the current transfer speed? You can jump to the customized dashboard of Amazon CloudWatch by clicking the CloudWatch Dashboard link in Task Detail of the web console. You can also go directly to CloudWatch to view it. 8. Do I need to create an S3 destination bucket before creating a transfer task? Yes, you need to create the destination S3 bucket in advance. 9. How to use Finder depth and Finder number to improve Finder performance? You can use these two parameters to increase the parallelism of Finder to improve the performance of data comparison. For example, if there are 12 subdirectories with over 100k files each, such as Jan , Feb , ..., Dec . You are recommended to set finderDepth =1 and finderNumber =12, so that your comparison performance will increase by 12 times. When using finderDepth and finderNumber, make sure that there are no objects in the folder whose level is equal to or less than finderDepth. Otherwise, data loss may occur. For example, assume that you set the finderDepth =2 and finderNumber =12 * 31 = 372, and your S3 bucket structure is like bucket_name/Jan/01/pic1.jpg . What will be lost are files like bucket_name/pic.jpg , bucket_name/Jan/pic.jpg . What will not be lost are all files under bucket_name/Jan/33/ , all files under bucket_name/13/33/ . 10. How to deal with Access Key Rotation? Currently, when Data Transfer Hub perceived that the Access Key of S3 has been rotated, it will fetch the latest key from AWS Secrets Manager automatically. Therefore, the Access Key Rotation will not affect the migrating process of DTH. Error messages After creating the task, you may encounter some error messages. The following list the error messages and provide general steps to troubleshoot them. 1. StatusCode: 400, InvalidToken: The provided token is malformed or otherwise invalid If you get this error message, confirm that your secret is configured in the following format. You can copy and paste it directly. { \"access_key_id\" : \"<Your Access Key ID>\" , \"secret_access_key\" : \"<Your Access Key Secret>\" } 2. StatusCode: 403, InvalidAccessKeyId: The AWS Access Key Id you provided does not exist in our records If you get this error message, check if your bucket name and region name are configured correctly. 3. StatusCode: 403, InvalidAccessKeyId: UnknownError If you get this error message, check whether the Credential stored in Secrets Manager has the proper permissions. For more information, refer to IAM Policy . 4. StatusCode: 400, AccessDeniedException: Access to KMS is not allowed If you get this error message, confirm that your secret is not encrypted by SSE-CMK. Currently, DTH does not support SSE-CMK encrypted secrets. 5. dial tcp: lookup xxx.xxxxx.xxxxx.xx (http://xxx.xxxxx.xxxxx.xx/) on xxx.xxx.xxx.xxx:53: no such host If you get this error message, check if your endpoint is configured correctly. Others 1. The cluster node (EC2) is terminated by mistake. How to resolve it? The Auto Scaling mechanism of the solution will enable automatic restart of a new working node. However, if a sharding task being transferred in the node is mistakenly terminated, it may cause that the files to which the shard belongs cannot be merged on the destination side, and the error \"api error NoSuchUpload: The specified upload does not exist. The upload ID may be invalid, or the upload may have been aborted or completed\" occurs. You need to configure lifecycle rules for Delete expired delete markers or incomplete multipart uploads in the Amazon S3 bucket. 2. The Secrets configuration in Secrets Manager is wrong. How to resolve it? You need to update Secrets in Secrets Manager first, and then go to the EC2 console to Terminate all EC2 instances that have been started by the task. Later, the Auto Scaling mechanism of the solution will automatically start a new working node and update Secrets to it. 3. How to find detailed transfer log? For Portal users Go to Tasks list page, and click the Task ID . You can see the dashboard and logs under the Monitoring section. Data Transfer Hub has embedded Dashboard and log groups on the Portal, so you do not need to navigate to AWS CloudWatch console to view the logs. For Plugin (Pure Backend) users When deploying the stack, you will be asked to enter the stack name ( DTHS3Stack by default), and most resources will be created with the name prefix as the stack name. For example, the format of the queue name is <StackName>-S3TransferQueue-<random suffix> . This plugin will create two main log groups. If there is no data transfer, you need to check whether there is a problem in the Finder task log. The following is the log group for scheduling Finder tasks. For more information, refer to the Error Code List section. <StackName>-EC2FinderLogGroup<random suffix> The following are the log groups of all EC2 instances, and you can find detailed transfer logs. <StackName>-EC2WorkerStackS3RepWorkerLogGroup<random suffix> 4. How to make customized build? If you want to make customized changes to this plugin, refer to Custom Build . 5. After the deployment is complete, why can't I find any log streams in the two CloudWatch log groups? This is because the subnet you selected when deploying this solution does not have public network access, and the EC2 cannot download the CloudWatch agent to send logs to CloudWatch. Check your VPC settings. After resolving the issue, you need to manually terminate the running EC2 instance (if any) through this solution. Later, the elastic scaling group will automatically start a new instance.","title":"FAQ"},{"location":"faq/#deployment","text":"1. In which AWS Regions can this solution be deployed? For the list of supported regions, refer to supported regions . 2. When creating a transfer task, shall I deploy it on the data source side or the destination side? The transfer performance of the solution will not be affected by whether the deployment is on the data source or destination side. If you do not have a domain name registered by ICP in AWS China Regions, we recommend you deploy it in AWS Standard Regions. If you need to deploy in AWS China Regions but do not have a domain name, you can directly deploy the back-end version: Amazon S3 Plugin: https://github.com/awslabs/amazon-s3-data-replication-hub-plugin Amazon ECR Plugin: https://github.com/awslabs/amazon-ecr-data-replication-hub-plugin 3. Do I need to deploy the solution on the data source and destination side separately? No. You can choose to deploy on the data source or destination side, which has no impact on the transfer performance. 4. Is it possible to deploy the solution in AWS account A and transfer Amazon S3 objects from account B to account C? Yes. In this case, you need to store the AccessKeyID and SecretAccessKey of account B and account C in the Secrets Manager of account A. 5. For data transfer within the production account, is it recommended to create an AWS account specifically for deploying the solution? Yes. It is recommended to create a new AWS account dedicated to deploying solutions. The account-level isolation improves the stability of the production account in the data synchronization process. 6. Is it possible to transfer data between different areas under the same account? Not supported currently. For this scenario, we recommend using Amazon S3's Cross-Region Replication . 7. Can I use AWS CLI to create a DTH S3 Transfer Task? Yes. Please refer to the tutorial Using AWS CLI to launch DTH S3 Transfer task .","title":"Deployment"},{"location":"faq/#performance","text":"1. Will there be any difference in data transfer performance for deployment in AWS China Regions and in AWS Standard Regions? No. If you do not have a domain name registered by ICP in AWS China Regions, it is recommended to deploy it in the AWS Standard Regions. 2. What are the factors influencing the data transfer performance? The transfer performance may be affected by average file size, destination of data transfer, geographic location of data source, and real-time network environment. For example, using the same configuration, the transfer speed with an average file size of 50MB is 170 times the transfer speed with an average file size of 10KB. 3. What is the scale up/scale down policy of Worker Auto Scaling Group? The Auto Scaling Group will automatically scale up or scale down according to the number of tasks in SQS. Scaling Up Steps are: { lower : 100 , cha n ge : + 1 } { lower : 500 , cha n ge : + 2 } { lower : 2000 , cha n ge : + 5 } { lower : 10000 , cha n ge : + 10 } Scaling Down Step is: { upper : 0 , cha n ge : -10000 }","title":"Performance"},{"location":"faq/#data-security-and-authentication","text":"1. How does the solution ensure data security? The solution adopts the following to ensure data security: All data is transferred in the memory in the transfer node cluster, without being placed on the disk. The external ports of all transfer nodes are closed, and there is no way to SSH into the transfer node. All data download and upload bottom layers are calling AWS official API, and data transfer conforms to the TLS protocol . 2. How does the solution ensure the security of resources on the cloud? In the research and development process, we strictly follow the minimum IAM permission design rules, and adopt the design of Auto Scaling, which will automatically help users terminate idle working nodes. 3. Is the front-end console open to the public network? How to ensure user authentication and multi-user management? Yes. You can access it with a front-end console link. User authentication and multi-user management are achieved through AWS Cognito User Pool in AWS Standard Regions, and through OIDC SAAS in AWS China Regions. 4. How does the solution achieve cross-account and cross-cloud authentication? By authentication through the Access Keyid and Access Key of the other party\u2019s account. The secret key is stored in AWS Secrets Manager and will be read in Secrets Manager as needed. 5. Does the solution support SSE-S3, SSE-KMS, and SSE-CMK? Yes. The solution supports the use of SSE-S3 and SSE-KMS data sources. If your source bucket has SSE-CMK enabled, refer to the tutorial .","title":"Data security and authentication"},{"location":"faq/#features","text":"1. What third-party clouds does Amazon S3 sync currently support? Alibaba Cloud OSS, Tencent Cloud, Huawei Cloud, Qiniu Cloud, Baidu Cloud, and all clouds that support S3 compatible protocols. 2. Why is the status of Task still in progress after all destination files are transferred? When will the task stop? For Fixed Rate Job The data difference between the data source and destination will be monitored continuously, and the differences between the two sides will be automatically compared after the first deployment. Moreover, when the default comparison task once an hour finds a difference, it will also transfer the difference data. Therefore, the status of the Task will always be in progress, unless the user manually terminates the task. Based on the built-in automatic expansion function of the solution, when there is no data to be transferred, the number of transfer working nodes will be automatically reduced to the minimum value configured by the user. For One Time Transfer Job When the objects are all transferred to the destination, the status of one time transfer job will become Completed . The transfer action will stop and you can select Stop to delete and release all backend resources. 3. How often will the data difference between the data source and destination be compared\uff1f By default, it runs hourly. At Task Scheduling Settings , you can make the task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . 4. Is it possible for real-time synchronization of newly added files? Near-real-time synchronization can be achieved, only if the Data Transfer Hub is deployed in the same AWS account and the same region as the data source. If the data source and the solution are not in the same account, you can configure it manually. For more information, refer to the tutorial . 5. Are there restrictions on the number of files and the size of files? No. Larger files will be uploaded in chunks. 6. If a single file transfer fails due to network issues, how to resolve it? Is there an error handling mechanism? There will be 5 retries. After 5 retries without success, the task will be notified to the user via email. 7. How to monitor the progress of the transfer by checking information like how many files are waiting to be transferred and the current transfer speed? You can jump to the customized dashboard of Amazon CloudWatch by clicking the CloudWatch Dashboard link in Task Detail of the web console. You can also go directly to CloudWatch to view it. 8. Do I need to create an S3 destination bucket before creating a transfer task? Yes, you need to create the destination S3 bucket in advance. 9. How to use Finder depth and Finder number to improve Finder performance? You can use these two parameters to increase the parallelism of Finder to improve the performance of data comparison. For example, if there are 12 subdirectories with over 100k files each, such as Jan , Feb , ..., Dec . You are recommended to set finderDepth =1 and finderNumber =12, so that your comparison performance will increase by 12 times. When using finderDepth and finderNumber, make sure that there are no objects in the folder whose level is equal to or less than finderDepth. Otherwise, data loss may occur. For example, assume that you set the finderDepth =2 and finderNumber =12 * 31 = 372, and your S3 bucket structure is like bucket_name/Jan/01/pic1.jpg . What will be lost are files like bucket_name/pic.jpg , bucket_name/Jan/pic.jpg . What will not be lost are all files under bucket_name/Jan/33/ , all files under bucket_name/13/33/ . 10. How to deal with Access Key Rotation? Currently, when Data Transfer Hub perceived that the Access Key of S3 has been rotated, it will fetch the latest key from AWS Secrets Manager automatically. Therefore, the Access Key Rotation will not affect the migrating process of DTH.","title":"Features"},{"location":"faq/#error-messages","text":"After creating the task, you may encounter some error messages. The following list the error messages and provide general steps to troubleshoot them. 1. StatusCode: 400, InvalidToken: The provided token is malformed or otherwise invalid If you get this error message, confirm that your secret is configured in the following format. You can copy and paste it directly. { \"access_key_id\" : \"<Your Access Key ID>\" , \"secret_access_key\" : \"<Your Access Key Secret>\" } 2. StatusCode: 403, InvalidAccessKeyId: The AWS Access Key Id you provided does not exist in our records If you get this error message, check if your bucket name and region name are configured correctly. 3. StatusCode: 403, InvalidAccessKeyId: UnknownError If you get this error message, check whether the Credential stored in Secrets Manager has the proper permissions. For more information, refer to IAM Policy . 4. StatusCode: 400, AccessDeniedException: Access to KMS is not allowed If you get this error message, confirm that your secret is not encrypted by SSE-CMK. Currently, DTH does not support SSE-CMK encrypted secrets. 5. dial tcp: lookup xxx.xxxxx.xxxxx.xx (http://xxx.xxxxx.xxxxx.xx/) on xxx.xxx.xxx.xxx:53: no such host If you get this error message, check if your endpoint is configured correctly.","title":"Error messages"},{"location":"faq/#others","text":"1. The cluster node (EC2) is terminated by mistake. How to resolve it? The Auto Scaling mechanism of the solution will enable automatic restart of a new working node. However, if a sharding task being transferred in the node is mistakenly terminated, it may cause that the files to which the shard belongs cannot be merged on the destination side, and the error \"api error NoSuchUpload: The specified upload does not exist. The upload ID may be invalid, or the upload may have been aborted or completed\" occurs. You need to configure lifecycle rules for Delete expired delete markers or incomplete multipart uploads in the Amazon S3 bucket. 2. The Secrets configuration in Secrets Manager is wrong. How to resolve it? You need to update Secrets in Secrets Manager first, and then go to the EC2 console to Terminate all EC2 instances that have been started by the task. Later, the Auto Scaling mechanism of the solution will automatically start a new working node and update Secrets to it. 3. How to find detailed transfer log? For Portal users Go to Tasks list page, and click the Task ID . You can see the dashboard and logs under the Monitoring section. Data Transfer Hub has embedded Dashboard and log groups on the Portal, so you do not need to navigate to AWS CloudWatch console to view the logs. For Plugin (Pure Backend) users When deploying the stack, you will be asked to enter the stack name ( DTHS3Stack by default), and most resources will be created with the name prefix as the stack name. For example, the format of the queue name is <StackName>-S3TransferQueue-<random suffix> . This plugin will create two main log groups. If there is no data transfer, you need to check whether there is a problem in the Finder task log. The following is the log group for scheduling Finder tasks. For more information, refer to the Error Code List section. <StackName>-EC2FinderLogGroup<random suffix> The following are the log groups of all EC2 instances, and you can find detailed transfer logs. <StackName>-EC2WorkerStackS3RepWorkerLogGroup<random suffix> 4. How to make customized build? If you want to make customized changes to this plugin, refer to Custom Build . 5. After the deployment is complete, why can't I find any log streams in the two CloudWatch log groups? This is because the subnet you selected when deploying this solution does not have public network access, and the EC2 cannot download the CloudWatch agent to send logs to CloudWatch. Check your VPC settings. After resolving the issue, you need to manually terminate the running EC2 instance (if any) through this solution. Later, the elastic scaling group will automatically start a new instance.","title":"Others"},{"location":"regions/","text":"This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List. Supported regions for deployment in AWS Standard Regions Region Name Region ID US East (N. Virginia) us-east-1 US East (Ohio) us-east-2 US West (N. California) us-west-1 US West (Oregon) us-west-2 Asia Pacific (Mumbai) ap-south-1 Asia Pacific (Tokyo) ap-northeast-1 Asia Pacific (Seoul) ap-northeast-2 Asia Pacific (Singapore) ap-southeast-1 Asia Pacific (Sydney) ap-southeast-2 Canada (Central) ca-central-1 Europe (Ireland) eu-west-1 Europe (London) eu-west-2 Europe (Stockholm) eu-north-1 Europe (Frankfurt) eu-central-1 South America (S\u00e3o Paulo) sa-east-1 Supported regions for deployment in AWS China Regions Region Name Region ID China (Beijing) Region Operated by Sinnet cn-north-1 China (Ningxia) Region Operated by NWCD cn-northwest-1","title":"Supported regions"},{"location":"regions/#supported-regions-for-deployment-in-aws-standard-regions","text":"Region Name Region ID US East (N. Virginia) us-east-1 US East (Ohio) us-east-2 US West (N. California) us-west-1 US West (Oregon) us-west-2 Asia Pacific (Mumbai) ap-south-1 Asia Pacific (Tokyo) ap-northeast-1 Asia Pacific (Seoul) ap-northeast-2 Asia Pacific (Singapore) ap-southeast-1 Asia Pacific (Sydney) ap-southeast-2 Canada (Central) ca-central-1 Europe (Ireland) eu-west-1 Europe (London) eu-west-2 Europe (Stockholm) eu-north-1 Europe (Frankfurt) eu-central-1 South America (S\u00e3o Paulo) sa-east-1","title":"Supported regions for deployment in AWS Standard Regions"},{"location":"regions/#supported-regions-for-deployment-in-aws-china-regions","text":"Region Name Region ID China (Beijing) Region Operated by Sinnet cn-north-1 China (Ningxia) Region Operated by NWCD cn-northwest-1","title":"Supported regions for deployment in AWS China Regions"},{"location":"revisions/","text":"Date Description January 2021 Initial release of version 1.0 July 2021 Released version 2.0 1. Support general OIDC providers, including Authing, Auth0, okta, etc. 2. Support transferring objects from more Amazon S3 compatible storage services, such as Huawei Cloud OBS. 3. Support setting the access control list (ACL) of the target bucket object 4. Support deployment in account A, and copying data from account B to account C 5. Change to use Graviton 2 instance, and turn on BBR to transfer S3 objects to improve performance and save costs 6. Change to use Secrets Manager to maintain credential information December 2021 Released version 2.1 1. Support custom prefix list to filter transfer tasks 2. Support configuration of single-run file transfer tasks 3. Support configuration of tasks through custom CRON Expression timetable 4. Support manual enabling or disabling of data comparison function July 2022 Released version 2.2 1. Support transfer data through Direct Connect December 2022 Released version 2.3 1. Support embeded dashboard and logs 2. Support S3 Access Key Rotation 3. Enhance One Time Transfer Task monitoring","title":"Revisions"},{"location":"security/","text":"When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, see AWS Cloud Security . IAM roles AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions, Amazon API Gateway and Amazon Cognito access to create regional resources. Amazon CloudFront This solution deploys a web console hosted in an Amazon Simple Storage Service (Amazon S3) bucket. To help reduce latency and improve security, this solution includes an Amazon CloudFront distribution with an origin access identity, which is a CloudFront user that provides public access to the solution\u2019s website bucket contents. For more information, refer to Restricting Access to Amazon S3 Content by Using an Origin Access Identity in the Amazon CloudFront Developer Guide.","title":"Security"},{"location":"security/#iam-roles","text":"AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions, Amazon API Gateway and Amazon Cognito access to create regional resources.","title":"IAM roles"},{"location":"security/#amazon-cloudfront","text":"This solution deploys a web console hosted in an Amazon Simple Storage Service (Amazon S3) bucket. To help reduce latency and improve security, this solution includes an Amazon CloudFront distribution with an origin access identity, which is a CloudFront user that provides public access to the solution\u2019s website bucket contents. For more information, refer to Restricting Access to Amazon S3 Content by Using an Origin Access Identity in the Amazon CloudFront Developer Guide.","title":"Amazon CloudFront"},{"location":"solution-components/","text":"This solution has three components: Web console Amazon S3 Transfer Engine Amazon ECR Transfer Engine Web console This solution provides a simple web console which allows you to create and manage transfer tasks for Amazon S3 and Amazon ECR. Amazon S3 Transfer Engine Amazon S3 transfer engine runs the Amazon S3 plugin and is used for transferring objects from their sources into S3 buckets. The S3 plugin supports the following features: Transfer Amazon S3 objects between AWS China Regions and AWS Regions Transfer objects from Alibaba Cloud OSS / Tencent COS / Qiniu Kodo to Amazon S3 Transfer objects from S3 Compatible Storage service to Amazon S3 Support near real time transfer via S3 Event Support Transfer with object metadata Support incremental data transfer Auto retry and error handling Amazon ECR Transfer Engine Amazon ECR engine runs the Amazon ECR plugin and is used for transferring container images from other container registries. The ECR plugin supports the following features: Transfer Amazon ECR images between AWS China Regions and AWS Regions Transfer from public container registry (such as Docker Hub, GCR.io, Quay.io) to Amazon ECR Transfer selected images to Amazon ECR Transfer all images and tags from Amazon ECR The ECR plugin leverages skopeo for the underlying engine. The AWS Lambda function lists images in their sources and uses Fargate to run the transfer jobs.","title":"Solution components"},{"location":"solution-components/#web-console","text":"This solution provides a simple web console which allows you to create and manage transfer tasks for Amazon S3 and Amazon ECR.","title":"Web console"},{"location":"solution-components/#amazon-s3-transfer-engine","text":"Amazon S3 transfer engine runs the Amazon S3 plugin and is used for transferring objects from their sources into S3 buckets. The S3 plugin supports the following features: Transfer Amazon S3 objects between AWS China Regions and AWS Regions Transfer objects from Alibaba Cloud OSS / Tencent COS / Qiniu Kodo to Amazon S3 Transfer objects from S3 Compatible Storage service to Amazon S3 Support near real time transfer via S3 Event Support Transfer with object metadata Support incremental data transfer Auto retry and error handling","title":"Amazon S3 Transfer Engine"},{"location":"solution-components/#amazon-ecr-transfer-engine","text":"Amazon ECR engine runs the Amazon ECR plugin and is used for transferring container images from other container registries. The ECR plugin supports the following features: Transfer Amazon ECR images between AWS China Regions and AWS Regions Transfer from public container registry (such as Docker Hub, GCR.io, Quay.io) to Amazon ECR Transfer selected images to Amazon ECR Transfer all images and tags from Amazon ECR The ECR plugin leverages skopeo for the underlying engine. The AWS Lambda function lists images in their sources and uses Fargate to run the transfer jobs.","title":"Amazon ECR Transfer Engine"},{"location":"source/","text":"Visit our GitHub repository to download the source files for this solution and to share your customizations with others. The Data Transfer Hub templates are generated using the AWS Cloud Development Kit (AWS CDK). Refer to the README.md file for additional information.","title":"Source code"},{"location":"template/","text":"To automate deployment, this solution uses the following AWS CloudFormation templates, which you can download before deployment: DataTransferHub-cognito.template : Use this template to launch the solution and all associated components in AWS Standard Regions where Amazon Cognito is available. The default configuration deploys Amazon S3, Amazon CloudFront, AWS AppSync, Amazon DynamoDB, AWS Lambda, Amazon ECS, and Amazon Cognito, but you can customize the template to meet your specific needs. DataTransferHub-openid.template : Use this template to launch the solution and all associated components in AWS China Regions where Amazon Cognito is not available. The default configuration deploys Amazon S3, Amazon CloudFront, AWS AppSync, Amazon DynamoDB, AWS Lambda, and Amazon ECS, but you can customize the template to meet your specific needs.","title":"CloudFormation template"},{"location":"tutorial-cli-launch/","text":"You can use the AWS CLI to create an Amazon S3 transfer task. Note that if you have deployed the DTH Portal at the same time, the tasks started through the CLI will not appear in the Task List on your Portal. Create an Amazon VPC with two public subnets or two private subnets with NAT gateway . Replace <CLOUDFORMATION_URL> as shown below. Global Region: https://aws-gcr-solutions.s3.amazonaws.com/data-transfer-hub-s3/latest/DataTransferS3Stack-ec2.template China Region: https://aws-gcr-solutions.s3.cn-north-1.amazonaws.com.cn/data-transfer-hub-s3/latest/DataTransferS3Stack-ec2.template Go to your terminal and enter the following command. For the parameter details, refer to the table below. aws cloudformation create-stack --stack-name dth-s3-task --template-url CLOUDFORMATION_URL \\ --capabilities CAPABILITY_NAMED_IAM \\ --parameters \\ ParameterKey = alarmEmail,ParameterValue = your_email@abc.com \\ ParameterKey = destBucket,ParameterValue = dth-receive-cn-north-1 \\ ParameterKey = destPrefix,ParameterValue = test-prefix \\ ParameterKey = destCredentials,ParameterValue = drh-cn-secret-key \\ ParameterKey = destInCurrentAccount,ParameterValue = false \\ ParameterKey = destRegion,ParameterValue = cn-north-1 \\ ParameterKey = destStorageClass,ParameterValue = STANDARD \\ ParameterKey = srcBucket,ParameterValue = dth-us-west-2 \\ ParameterKey = srcInCurrentAccount,ParameterValue = true \\ ParameterKey = srcCredentials,ParameterValue = \\ ParameterKey = srcRegion,ParameterValue = us-west-2 \\ ParameterKey = srcPrefix,ParameterValue = case1 \\ ParameterKey = srcType,ParameterValue = Amazon_S3 \\ ParameterKey = ec2VpcId,ParameterValue = vpc-040bbab85f0e4e088 \\ ParameterKey = ec2Subnets,ParameterValue = subnet-0d1bf2725ab8e94ee \\\\ ,subnet-06d17b2b3286be40e \\ ParameterKey = finderEc2Memory,ParameterValue = 8 \\ ParameterKey = ec2CronExpression,ParameterValue = \"0/60 * * * ? *\" \\ ParameterKey = includeMetadata,ParameterValue = false \\ ParameterKey = srcEvent,ParameterValue = No \\ ParameterKey = maxCapacity,ParameterValue = 20 \\ ParameterKey = minCapacity,ParameterValue = 1 \\ ParameterKey = desiredCapacity,ParameterValue = 1 Parameters Parameter Name Allowed Value Default Value Description alarmEmail An email to which errors will be sent desiredCapacity 1 Desired capacity for Auto Scaling Group destAcl private public-read public-read-write authenticated-read aws-exec-read bucket-owner-read bucket-owner-full-control bucket-owner-full-control Destination access control list destBucket Destination bucket name destCredentials Secret name in Secrets Manager used to keep AK/SK credentials for destination bucket. Leave it blank if the destination bucket is in the current account destInCurrentAccount true false true Indicates whether the destination bucket is in current account. If not, you should provide a credential with read and write access destPrefix Destination prefix (Optional) destRegion Destination region name destStorageClass STANDARD STANDARD_IA ONEZONE_IA INTELLIGENT_TIERING INTELLIGENT_TIERING Destination storage class, which defaults to INTELLIGENT_TIERING ec2CronExpression 0/60 * * * ? * Cron expression for EC2 Finder task \"\" for one time transfer. finderEc2Memory 8 16 32 64 128 256 8 GB The amount of memory (in GB) used by the Finder task. ec2Subnets Two public subnets or two private subnets with NAT gateway ec2VpcId VPC ID to run EC2 task, for example, vpc-bef13dc7 finderDepth 0 Depth of sub folders to compare in parallel. 0 means comparing all objects in sequence finderNumber 1 The number of finder threads to run in parallel includeMetadata true false false Indicates whether to add replication of object metadata. If true, there will be additional API calls. maxCapacity 20 Maximum capacity for Auto Scaling Group minCapacity 1 Minimum capacity for Auto Scaling Group srcBucket Source bucket name srcCredentials Secret name in Secrets Manager used to keep AK/SK credentials for Source Bucket. Leave it blank if source bucket is in the current account or source is open data srcEndpoint Source Endpoint URL (Optional). Leave it blank unless you want to provide a custom Endpoint URL srcEvent No Create CreateAndDelete No Whether to enable S3 Event to trigger the replication. Note that S3Event is only applicable if source is in the current account srcInCurrentAccount true false false Indicates whether the source bucket is in the current account. If not, you should provide a credential with read access srcPrefix Source prefix (Optional) srcPrefixsListFile Source prefix list file S3 path (Optional). It supports txt type, for example, my_prefix_list.txt, and the maximum number of lines is 10 millions srcRegion Source region name srcSkipCompare true false false Indicates whether to skip the data comparison in task finding process. If yes, all data in the source will be sent to the destination srcType Amazon_S3 Aliyun_OSS Qiniu_Kodo Tencent_COS Amazon_S3 If you choose to use the Endpoint mode, please select Amazon_S3. workerNumber 1 ~ 10 4 The number of worker threads to run in one worker node/instance. For small files (size < 1MB), you can increase the number of workers to improve the transfer performance.","title":"Using AWS CLI to create S3 transfer task"},{"location":"tutorial-cli-launch/#parameters","text":"Parameter Name Allowed Value Default Value Description alarmEmail An email to which errors will be sent desiredCapacity 1 Desired capacity for Auto Scaling Group destAcl private public-read public-read-write authenticated-read aws-exec-read bucket-owner-read bucket-owner-full-control bucket-owner-full-control Destination access control list destBucket Destination bucket name destCredentials Secret name in Secrets Manager used to keep AK/SK credentials for destination bucket. Leave it blank if the destination bucket is in the current account destInCurrentAccount true false true Indicates whether the destination bucket is in current account. If not, you should provide a credential with read and write access destPrefix Destination prefix (Optional) destRegion Destination region name destStorageClass STANDARD STANDARD_IA ONEZONE_IA INTELLIGENT_TIERING INTELLIGENT_TIERING Destination storage class, which defaults to INTELLIGENT_TIERING ec2CronExpression 0/60 * * * ? * Cron expression for EC2 Finder task \"\" for one time transfer. finderEc2Memory 8 16 32 64 128 256 8 GB The amount of memory (in GB) used by the Finder task. ec2Subnets Two public subnets or two private subnets with NAT gateway ec2VpcId VPC ID to run EC2 task, for example, vpc-bef13dc7 finderDepth 0 Depth of sub folders to compare in parallel. 0 means comparing all objects in sequence finderNumber 1 The number of finder threads to run in parallel includeMetadata true false false Indicates whether to add replication of object metadata. If true, there will be additional API calls. maxCapacity 20 Maximum capacity for Auto Scaling Group minCapacity 1 Minimum capacity for Auto Scaling Group srcBucket Source bucket name srcCredentials Secret name in Secrets Manager used to keep AK/SK credentials for Source Bucket. Leave it blank if source bucket is in the current account or source is open data srcEndpoint Source Endpoint URL (Optional). Leave it blank unless you want to provide a custom Endpoint URL srcEvent No Create CreateAndDelete No Whether to enable S3 Event to trigger the replication. Note that S3Event is only applicable if source is in the current account srcInCurrentAccount true false false Indicates whether the source bucket is in the current account. If not, you should provide a credential with read access srcPrefix Source prefix (Optional) srcPrefixsListFile Source prefix list file S3 path (Optional). It supports txt type, for example, my_prefix_list.txt, and the maximum number of lines is 10 millions srcRegion Source region name srcSkipCompare true false false Indicates whether to skip the data comparison in task finding process. If yes, all data in the source will be sent to the destination srcType Amazon_S3 Aliyun_OSS Qiniu_Kodo Tencent_COS Amazon_S3 If you choose to use the Endpoint mode, please select Amazon_S3. workerNumber 1 ~ 10 4 The number of worker threads to run in one worker node/instance. For small files (size < 1MB), you can increase the number of workers to improve the transfer performance.","title":"Parameters"},{"location":"tutorial-directconnect/","text":"This tutorial describes how to use Data Transfer Hub (DTH) via Direct Connect (DX). When DTH worker node and finder node start to work, they need to download related assets (such as CloudWatch agent, DTH CLI) from internet by default. In an isolated network, you need to manually download and upload these files to an S3 bucket in the region where DTH is deployed. You have two options to use DTH to transfer data via DX: Use DTH to transfer data via DX in a non-isolated network Use DTH to transfer data via DX in an isolated network Use DTH to transfer data via DX in a non-isolated network In this scenario, DTH is deployed in the destination side and within a VPC with public access (has Internet Gateway or NAT), and the source bucket is in the isolated network. Note As DTH deployment VPC has public internet access (IGW or NAT), EC2 worker/finder nodes can access other AWS services used by DTH such as secret managers and download related assets (such as CloudWatch agent, DTH CLI) from internet without any changes. From the Create Transfer Task page, select Create New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source Amazon S3 Compatible Storage . Enter endpoint url , which must be the interface endpoint url, such as https://bucket.vpce-076205013d3a9a2ca-us23z2ze.s3.ap-east-1.vpce.amazonaws.com . You can find the specific url in VPC Endpoint Console DNS names part. Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . Provide destination settings for the S3 buckets. From Engine settings , verify the values and modify them if necessary. For incremental data transfer, we recommend to set the minimum capacity to at least 1. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . For Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. If you want to skip the data comparison process and transfer all files, select No . If you only want to synchronize files with differences, select Yes . In Alarm Email , provide an email address. Choose Next and review your task parameter details. Choose Create Task . Use DTH to transfer data via DX in an isolated network In this scenario, DTH is deployed in the destination side and within a VPC without public access (isolated VPC), and the source bucket is also in an isolated network. Prerequisites Configure the service endpoints for VPC DTH worker/finder nodes need to access other AWS services. To do so, you need to create Gateway Endpoint for DynamoDB and S3 , create Interface Endpoint for logs , SQS and Secret Managers . Upload the artifacts to an S3 bucket In an isolated network, do the following to manually download and upload files to an S3 bucket in the region where DTH is deployed. Download Amazon CloudWatch Agent and DTH CLI . Create the worker's CloudWatch Agent Config file. You can create a file named cw_agent_config.json . { \"agent\" : { \"metrics_collection_interval\" : 60 , \"run_as_user\" : \"root\" }, \"logs\" : { \"logs_collected\" : { \"files\" : { \"collect_list\" : [ { \"file_path\" : \"/home/ec2-user/worker.log\" , \"log_group_name\" : \"##log group##\" , \"log_stream_name\" : \"Instance-{instance_id}\" } ] } } }, \"metrics\" : { \"append_dimensions\" : { \"AutoScalingGroupName\" : \"${aws:AutoScalingGroupName}\" , \"InstanceId\" : \"${aws:InstanceId}\" }, \"aggregation_dimensions\" : [ [ \"AutoScalingGroupName\" ] ], \"metrics_collected\" : { \"disk\" : { \"measurement\" : [ \"used_percent\" ], \"metrics_collection_interval\" : 60 , \"resources\" : [ \"*\" ] }, \"mem\" : { \"measurement\" : [ \"mem_used_percent\" ], \"metrics_collection_interval\" : 60 } } } } Upload these three files to an S3 bucket in the region where DTH is deployed. Deploy the DTH S3-Plugin We recommend using the DTH S3-plugin to create the transfer task, instead of using the DTH console. For AWS China Regions For AWS Global Regions For Source Type , choose Amazon_S3 . Enter the Source Bucket name. Enter the Source Prefix if needed. Enter the Source Endpoint URL . For example, https://bucket.vpce-076205013d3a9a2ca-us23z2ze.s3.ap-east-1.vpce.amazonaws.com . For Source In Current Account , choose false . For Source Credentials , enter the secret's name stored in the Secrets Manager . For Enable S3 Event , choose No . Configure the Destination Bucket , Destination Prefix , Destination Region and Destination in Current Account . Leave the Destination Credentials blank if the destination bucket is in current account. Configure the Alarm Email . Configure the VPC ID and Subnet IDs . For other parameters, keep the default values and choose Next . Choose Next . Configure additional stack options such as tags (Optional). Choose Next . Review and confirm acknowledgement, then choose Create Stack to start the deployment. The deployment will take approximately 3 to 5 minutes. Update the EC2 Userdata for worker nodes and finder node Update worker nodes' Userdata Go to the Auto Scaling Group's Launch configurations . Select the configuration and choose Copy Launch Configuration . Edit the User data under the Advanced details section. Replace the code above echo \"export JOB_TABLE_NAME=xxxxxxxxxxx\" >> env.sh with the following shell script. #!/bin/bash yum update -y cd /home/ec2-user/ asset_bucket = <asset_bucket_name> aws s3 cp \"s3:// $asset_bucket /cw_agent_config.json\" . --region <deploy_region_name> aws s3 cp \"s3:// $asset_bucket /amazon-cloudwatch-agent.rpm\" . --region <deploy_region_name> aws s3 cp \"s3:// $asset_bucket /dthcli_1.0.1_linux_arm64.tar.gz\" . --region <deploy_region_name> sudo yum install -y amazon-cloudwatch-agent.rpm sed -i -e \"s/##log group##/<worker_log_group_name>/g\" cw_agent_config.json /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/home/ec2-user/cw_agent_config.json -s tar zxvf dthcli_1.0.1_linux_arm64.tar.gz Replace the <asset_bucket_name> with your specific bucket name where the assets are stored. Replace the <deploy_region_name> with the region where you deploy the DTH S3-Plugin solution. Replace the <worker_log_group_name> with the DTH Worker's log group name. Do not edit the code behind echo \"export JOB_TABLE_NAME=xxxxxxxxxxx\" >> env.sh . Choose Create Launch Configuration . Go to Auto Scaling Group . Choose the specific scaling group and click Edit . In the Launch configuration section, choose the new launch configuration created in the previous step. Click Update . Terminate all the running DTH worker node, and the Auto Scaling Group will launch the new worker node with the new Userdata. Update finder nodes' Userdata Go to the EC2 Launch Templates . Click Modify template . Edit the User data under the Advanced details section. Replace the code above echo \"export JOB_TABLE_NAME=xxxxxxxxxxx\" >> env.sh using the shell script bellow. #!/bin/bash yum update -y cd /home/ec2-user/ asset_bucket = <asset_bucket_name> aws s3 cp \"s3:// $asset_bucket /amazon-cloudwatch-agent.rpm\" . --region <deploy_region_name> aws s3 cp \"s3:// $asset_bucket /dthcli_1.0.1_linux_arm64.tar.gz\" . --region <deploy_region_name> echo \"{\\\"agent\\\": {\\\"metrics_collection_interval\\\": 60,\\\"run_as_user\\\": \\\"root\\\"},\\\"logs\\\": {\\\"logs_collected\\\": {\\\"files\\\": {\\\"collect_list\\\": [{\\\"file_path\\\": \\\"/home/ec2-user/finder.log\\\",\\\"log_group_name\\\": \\\"##log group##\\\"}]}}}}\" >> /home/ec2-user/cw_agent_config.json, sudo yum install -y amazon-cloudwatch-agent.rpm sed -i -e \"s/##log group##/<finder_log_group_name>`/g\" cw_agent_config.json /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/home/ec2-user/cw_agent_config.json -s tar zxvf dthcli_1.0.1_linux_arm64.tar.gz Replace the <asset_bucket_name> with your specific bucket name where the assets are stored. Replace the <deploy_region_name> with the region where you deploy the DTH S3-Plugin solution. Replace the <finder_log_group_name> with the DTH Finder's log group name. Do not edit the code behind echo \"export JOB_TABLE_NAME=xxxxxxxxxxx\" >> env.sh . Choose Create template version . Use this new version template to launch a new Finder Node, and manually terminate the old one. Architecture DTH worker nodes running on EC2 transfer data from bucket in one AWS account to bucket in another AWS account. To access bucket in the account where DTH is deployed, DTH worker nodes use S3 Gateway Endpoint To access bucket in another account, DTH worker nodes use S3 Private Link by S3 Interface Endpoint","title":"Transfer S3 object via Direct Connect"},{"location":"tutorial-directconnect/#use-dth-to-transfer-data-via-dx-in-a-non-isolated-network","text":"In this scenario, DTH is deployed in the destination side and within a VPC with public access (has Internet Gateway or NAT), and the source bucket is in the isolated network. Note As DTH deployment VPC has public internet access (IGW or NAT), EC2 worker/finder nodes can access other AWS services used by DTH such as secret managers and download related assets (such as CloudWatch agent, DTH CLI) from internet without any changes. From the Create Transfer Task page, select Create New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source Amazon S3 Compatible Storage . Enter endpoint url , which must be the interface endpoint url, such as https://bucket.vpce-076205013d3a9a2ca-us23z2ze.s3.ap-east-1.vpce.amazonaws.com . You can find the specific url in VPC Endpoint Console DNS names part. Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . Provide destination settings for the S3 buckets. From Engine settings , verify the values and modify them if necessary. For incremental data transfer, we recommend to set the minimum capacity to at least 1. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . For Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. If you want to skip the data comparison process and transfer all files, select No . If you only want to synchronize files with differences, select Yes . In Alarm Email , provide an email address. Choose Next and review your task parameter details. Choose Create Task .","title":"Use DTH to transfer data via DX in a non-isolated network "},{"location":"tutorial-directconnect/#use-dth-to-transfer-data-via-dx-in-an-isolated-network","text":"In this scenario, DTH is deployed in the destination side and within a VPC without public access (isolated VPC), and the source bucket is also in an isolated network.","title":"Use DTH to transfer data via DX in an isolated network "},{"location":"tutorial-directconnect/#prerequisites","text":"Configure the service endpoints for VPC DTH worker/finder nodes need to access other AWS services. To do so, you need to create Gateway Endpoint for DynamoDB and S3 , create Interface Endpoint for logs , SQS and Secret Managers . Upload the artifacts to an S3 bucket In an isolated network, do the following to manually download and upload files to an S3 bucket in the region where DTH is deployed. Download Amazon CloudWatch Agent and DTH CLI . Create the worker's CloudWatch Agent Config file. You can create a file named cw_agent_config.json . { \"agent\" : { \"metrics_collection_interval\" : 60 , \"run_as_user\" : \"root\" }, \"logs\" : { \"logs_collected\" : { \"files\" : { \"collect_list\" : [ { \"file_path\" : \"/home/ec2-user/worker.log\" , \"log_group_name\" : \"##log group##\" , \"log_stream_name\" : \"Instance-{instance_id}\" } ] } } }, \"metrics\" : { \"append_dimensions\" : { \"AutoScalingGroupName\" : \"${aws:AutoScalingGroupName}\" , \"InstanceId\" : \"${aws:InstanceId}\" }, \"aggregation_dimensions\" : [ [ \"AutoScalingGroupName\" ] ], \"metrics_collected\" : { \"disk\" : { \"measurement\" : [ \"used_percent\" ], \"metrics_collection_interval\" : 60 , \"resources\" : [ \"*\" ] }, \"mem\" : { \"measurement\" : [ \"mem_used_percent\" ], \"metrics_collection_interval\" : 60 } } } } Upload these three files to an S3 bucket in the region where DTH is deployed.","title":"Prerequisites"},{"location":"tutorial-directconnect/#deploy-the-dth-s3-plugin","text":"We recommend using the DTH S3-plugin to create the transfer task, instead of using the DTH console. For AWS China Regions For AWS Global Regions For Source Type , choose Amazon_S3 . Enter the Source Bucket name. Enter the Source Prefix if needed. Enter the Source Endpoint URL . For example, https://bucket.vpce-076205013d3a9a2ca-us23z2ze.s3.ap-east-1.vpce.amazonaws.com . For Source In Current Account , choose false . For Source Credentials , enter the secret's name stored in the Secrets Manager . For Enable S3 Event , choose No . Configure the Destination Bucket , Destination Prefix , Destination Region and Destination in Current Account . Leave the Destination Credentials blank if the destination bucket is in current account. Configure the Alarm Email . Configure the VPC ID and Subnet IDs . For other parameters, keep the default values and choose Next . Choose Next . Configure additional stack options such as tags (Optional). Choose Next . Review and confirm acknowledgement, then choose Create Stack to start the deployment. The deployment will take approximately 3 to 5 minutes.","title":"Deploy the DTH S3-Plugin"},{"location":"tutorial-directconnect/#update-the-ec2-userdata-for-worker-nodes-and-finder-node","text":"Update worker nodes' Userdata Go to the Auto Scaling Group's Launch configurations . Select the configuration and choose Copy Launch Configuration . Edit the User data under the Advanced details section. Replace the code above echo \"export JOB_TABLE_NAME=xxxxxxxxxxx\" >> env.sh with the following shell script. #!/bin/bash yum update -y cd /home/ec2-user/ asset_bucket = <asset_bucket_name> aws s3 cp \"s3:// $asset_bucket /cw_agent_config.json\" . --region <deploy_region_name> aws s3 cp \"s3:// $asset_bucket /amazon-cloudwatch-agent.rpm\" . --region <deploy_region_name> aws s3 cp \"s3:// $asset_bucket /dthcli_1.0.1_linux_arm64.tar.gz\" . --region <deploy_region_name> sudo yum install -y amazon-cloudwatch-agent.rpm sed -i -e \"s/##log group##/<worker_log_group_name>/g\" cw_agent_config.json /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/home/ec2-user/cw_agent_config.json -s tar zxvf dthcli_1.0.1_linux_arm64.tar.gz Replace the <asset_bucket_name> with your specific bucket name where the assets are stored. Replace the <deploy_region_name> with the region where you deploy the DTH S3-Plugin solution. Replace the <worker_log_group_name> with the DTH Worker's log group name. Do not edit the code behind echo \"export JOB_TABLE_NAME=xxxxxxxxxxx\" >> env.sh . Choose Create Launch Configuration . Go to Auto Scaling Group . Choose the specific scaling group and click Edit . In the Launch configuration section, choose the new launch configuration created in the previous step. Click Update . Terminate all the running DTH worker node, and the Auto Scaling Group will launch the new worker node with the new Userdata. Update finder nodes' Userdata Go to the EC2 Launch Templates . Click Modify template . Edit the User data under the Advanced details section. Replace the code above echo \"export JOB_TABLE_NAME=xxxxxxxxxxx\" >> env.sh using the shell script bellow. #!/bin/bash yum update -y cd /home/ec2-user/ asset_bucket = <asset_bucket_name> aws s3 cp \"s3:// $asset_bucket /amazon-cloudwatch-agent.rpm\" . --region <deploy_region_name> aws s3 cp \"s3:// $asset_bucket /dthcli_1.0.1_linux_arm64.tar.gz\" . --region <deploy_region_name> echo \"{\\\"agent\\\": {\\\"metrics_collection_interval\\\": 60,\\\"run_as_user\\\": \\\"root\\\"},\\\"logs\\\": {\\\"logs_collected\\\": {\\\"files\\\": {\\\"collect_list\\\": [{\\\"file_path\\\": \\\"/home/ec2-user/finder.log\\\",\\\"log_group_name\\\": \\\"##log group##\\\"}]}}}}\" >> /home/ec2-user/cw_agent_config.json, sudo yum install -y amazon-cloudwatch-agent.rpm sed -i -e \"s/##log group##/<finder_log_group_name>`/g\" cw_agent_config.json /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/home/ec2-user/cw_agent_config.json -s tar zxvf dthcli_1.0.1_linux_arm64.tar.gz Replace the <asset_bucket_name> with your specific bucket name where the assets are stored. Replace the <deploy_region_name> with the region where you deploy the DTH S3-Plugin solution. Replace the <finder_log_group_name> with the DTH Finder's log group name. Do not edit the code behind echo \"export JOB_TABLE_NAME=xxxxxxxxxxx\" >> env.sh . Choose Create template version . Use this new version template to launch a new Finder Node, and manually terminate the old one.","title":"Update the EC2 Userdata for worker nodes and finder node"},{"location":"tutorial-directconnect/#architecture","text":"DTH worker nodes running on EC2 transfer data from bucket in one AWS account to bucket in another AWS account. To access bucket in the account where DTH is deployed, DTH worker nodes use S3 Gateway Endpoint To access bucket in another account, DTH worker nodes use S3 Private Link by S3 Interface Endpoint","title":"Architecture"},{"location":"tutorial-ecr/","text":"You can use the web console to create an Amazon ECR transfer task. For more information about how to launch the web console, see deployment . From the Create Transfer Task page, select Create New Task , and then select Next . From the Engine options page, under engine, select Amazon ECR , and then choose Next Step . You can also copy image from Docker Hub\uff0cGCR.io\uff0cQuay.io, and so on by choosing Public Container Registry . Specify the transfer task details. In Source Type , select the container warehouse type. In Source settings , enter Source Region and Amazon Web Services Account ID . To create credential information, select Secrets Manager to jump to the AWS Secrets Manager console in the current region. From the left menu, select Secrets , then choose Store a new secret and select the other type of secrets key type. Fill in the access_key_id and secret_access_key information in the Plaintext input box according to the displayed format. For more information, refer to IAM features in the IAM User Guide . Choose Next . (Optional) Enter the key name and description. Choose Next . In the configuration of automatic rotation, select Disable automatic rotation. Choose Next . Keep the default value and choose Save to complete the creation of the key. Navigate back to the Data Transfer Hub task creation interface and refresh the interface. Your new secret is displayed in the drop-down list. Select the certificate (Secret). Note If the source is in the same account with Data Transfer Hub deployment, you need to create/provide credential info for the destination. Otherwise, no credential information is needed. Enter an email address in Alarm Email . Choose Next and review your task parameter details. Choose Create Task . After the task is created successfully, it will appear on the Tasks page.","title":"Create Amazon ECR transfer task"},{"location":"tutorial-oss/","text":"This tutorial describes how to transfer Objects from Alibaba Cloud OSS to Amazon S3 . Prerequisite You have already deployed the Data Transfer Hub in Oregon (us-west-2) region. For more information, see deployment . Step 1: Configure credentials for OSS Open the Secrets Manager console. Choose Secrets in the left navigation bar. Click Store a new secret button. Select Other type of secrets . Enter the credentials of Alibaba Cloud as text in Plaintext . The credentials are in the format of: { \"access_key_id\": \"<Your Access Key ID>\", \"secret_access_key\": \"<Your Access Key Secret>\" } Click Next . Enter Secret name . For example, dth-oss-credentials . Click Next . Select Disable automatic rotation . Click Store . Step 2: Create an OSS transfer task From the Create Transfer Task page, select Create New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source Aliyun OSS . Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . Provide destination settings for the S3 buckets. From Engine settings , verify the values and modify them if necessary. For incremental data transfer, we recommend to set the minimum capacity to at least 1. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . If you need to achieve real-time incremental data synchronization, please refer to the event config guide . For Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. -If you want to skip the data comparison process and transfer all files, select No . -If you only want to synchronize files with differences, select Yes . Enter an email address in Alarm Email . Choose Next and review your task parameter details. Choose Create Task . After the task is created successfully, it will appear on the Tasks page. Figure 2: Transfer task details and status Select the Task ID to go to the task Details page, and then choose CloudWatch Dashboard to monitor the task status. How to achieve real-time data transfer by OSS event trigger If you want to achieve real-time data transfer from Alibaba Cloud OSS to Amazon S3, follow this section to enable OSS event trigger. After you created the task, go to SQS console and record the Queue URL and Queue arn that will be used later. Prepare your AWS account's Access Key/Secret Key Go to IAM console , and click Create Policy . Choose the JSON tab, and enter the following information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"sqs:SendMessageBatch\" , \"sqs:SendMessage\" ], \"Resource\" : \"arn:aws:sqs:us-west-2:xxxxxxxxxxx:DTHS3Stack-S3TransferQueue-1TSF4ESFQEFKJ\" } ] } Note Replace your Queue arn in the JSON. Create the user. Go to the User console and click Add User . Attach the policy you created previously to the user. Save the ACCESS_KEY/SECRET_KEY which will be used later. Prepare the event-sender function for Alibaba Cloud Open the terminal and enter the following command. You can use docker or Linux machine. mkdir tmp cd tmp pip3 install -t . boto3 Create a index.py in the same folder, and enter the code below. import json import logging import os import boto3 def handler ( event , context ): logger = logging . getLogger () logger . setLevel ( 'INFO' ) evt = json . loads ( event ) if 'events' in evt and len ( evt [ 'events' ]) == 1 : evt = evt [ 'events' ][ 0 ] logger . info ( 'Got event {} ' . format ( evt [ 'eventName' ])) obj = evt [ 'oss' ][ 'object' ] # logger.info(obj) ak = os . environ [ 'ACCESS_KEY' ] sk = os . environ [ 'SECRET_KEY' ] queue_url = os . environ [ 'QUEUE_URL' ] region_name = os . environ [ 'REGION_NAME' ] # minimum info of a message obj_msg = { 'key' : obj [ 'key' ], 'size' : obj [ 'size' ] } # start sending the msg sqs = boto3 . client ( 'sqs' , region_name = region_name , aws_access_key_id = ak , aws_secret_access_key = sk ) try : sqs . send_message ( QueueUrl = queue_url , MessageBody = json . dumps ( obj_msg ) ) except Exception as e : logger . error ( 'Unable to send the message to Amazon SQS, Exception:' , e ) else : logger . warning ( 'Unknown Message ' + evt ) return 'Done' Zip the code (including boto3). zip -r code.zip * Create a function in Alibaba Cloud Use your Alibaba Cloud account to log in to Function Compute , and click Task . Click Create Function . Choose Python3.x as the Runtime Environments . In Code Upload Method , choose Upload ZIP . Upload the code.zip created in the previous step to create the function. Click Create . Configure the function's environment variables Click the Configurations . Click Modify in the Environment Variables . Enter the config json in the Environment Variables . Here you need to use your own ACCESS_KEY , SECRET_KEY and QUEUE_URL . { \"ACCESS_KEY\" : \"XXX\" , \"QUEUE_URL\" : \"https://sqs.us-west-2.amazonaws.com/xxxx/DTHS3Stack-S3TransferQueue-xxxx\" , \"REGION_NAME\" : \"us-west-2\" , \"SECRET_KEY\" : \"XXXXX\" } 4. Click OK . Create the trigger Click the Create Trigger in Triggers tab to create the trigger for the function. Choose OSS as the Trigger Type , choose the bucket name. For Trigger Event , choose: oss:ObjectCreated:PutObject oss:ObjectCreated:PostObject oss:ObjectCreated:CopyObject oss:ObjectCreated:CompleteMultipartUpload oss:ObjectCreated:AppendObject Click OK .","title":"Transfer S3 object from Alibaba Cloud OSS"},{"location":"tutorial-oss/#prerequisite","text":"You have already deployed the Data Transfer Hub in Oregon (us-west-2) region. For more information, see deployment .","title":"Prerequisite"},{"location":"tutorial-oss/#step-1-configure-credentials-for-oss","text":"Open the Secrets Manager console. Choose Secrets in the left navigation bar. Click Store a new secret button. Select Other type of secrets . Enter the credentials of Alibaba Cloud as text in Plaintext . The credentials are in the format of: { \"access_key_id\": \"<Your Access Key ID>\", \"secret_access_key\": \"<Your Access Key Secret>\" } Click Next . Enter Secret name . For example, dth-oss-credentials . Click Next . Select Disable automatic rotation . Click Store .","title":"Step 1: Configure credentials for OSS"},{"location":"tutorial-oss/#step-2-create-an-oss-transfer-task","text":"From the Create Transfer Task page, select Create New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source Aliyun OSS . Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . Provide destination settings for the S3 buckets. From Engine settings , verify the values and modify them if necessary. For incremental data transfer, we recommend to set the minimum capacity to at least 1. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . If you need to achieve real-time incremental data synchronization, please refer to the event config guide . For Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. -If you want to skip the data comparison process and transfer all files, select No . -If you only want to synchronize files with differences, select Yes . Enter an email address in Alarm Email . Choose Next and review your task parameter details. Choose Create Task . After the task is created successfully, it will appear on the Tasks page. Figure 2: Transfer task details and status Select the Task ID to go to the task Details page, and then choose CloudWatch Dashboard to monitor the task status.","title":"Step 2: Create an OSS transfer task"},{"location":"tutorial-oss/#how-to-achieve-real-time-data-transfer-by-oss-event-trigger","text":"If you want to achieve real-time data transfer from Alibaba Cloud OSS to Amazon S3, follow this section to enable OSS event trigger. After you created the task, go to SQS console and record the Queue URL and Queue arn that will be used later. Prepare your AWS account's Access Key/Secret Key Go to IAM console , and click Create Policy . Choose the JSON tab, and enter the following information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"sqs:SendMessageBatch\" , \"sqs:SendMessage\" ], \"Resource\" : \"arn:aws:sqs:us-west-2:xxxxxxxxxxx:DTHS3Stack-S3TransferQueue-1TSF4ESFQEFKJ\" } ] } Note Replace your Queue arn in the JSON. Create the user. Go to the User console and click Add User . Attach the policy you created previously to the user. Save the ACCESS_KEY/SECRET_KEY which will be used later. Prepare the event-sender function for Alibaba Cloud Open the terminal and enter the following command. You can use docker or Linux machine. mkdir tmp cd tmp pip3 install -t . boto3 Create a index.py in the same folder, and enter the code below. import json import logging import os import boto3 def handler ( event , context ): logger = logging . getLogger () logger . setLevel ( 'INFO' ) evt = json . loads ( event ) if 'events' in evt and len ( evt [ 'events' ]) == 1 : evt = evt [ 'events' ][ 0 ] logger . info ( 'Got event {} ' . format ( evt [ 'eventName' ])) obj = evt [ 'oss' ][ 'object' ] # logger.info(obj) ak = os . environ [ 'ACCESS_KEY' ] sk = os . environ [ 'SECRET_KEY' ] queue_url = os . environ [ 'QUEUE_URL' ] region_name = os . environ [ 'REGION_NAME' ] # minimum info of a message obj_msg = { 'key' : obj [ 'key' ], 'size' : obj [ 'size' ] } # start sending the msg sqs = boto3 . client ( 'sqs' , region_name = region_name , aws_access_key_id = ak , aws_secret_access_key = sk ) try : sqs . send_message ( QueueUrl = queue_url , MessageBody = json . dumps ( obj_msg ) ) except Exception as e : logger . error ( 'Unable to send the message to Amazon SQS, Exception:' , e ) else : logger . warning ( 'Unknown Message ' + evt ) return 'Done' Zip the code (including boto3). zip -r code.zip * Create a function in Alibaba Cloud Use your Alibaba Cloud account to log in to Function Compute , and click Task . Click Create Function . Choose Python3.x as the Runtime Environments . In Code Upload Method , choose Upload ZIP . Upload the code.zip created in the previous step to create the function. Click Create . Configure the function's environment variables Click the Configurations . Click Modify in the Environment Variables . Enter the config json in the Environment Variables . Here you need to use your own ACCESS_KEY , SECRET_KEY and QUEUE_URL . { \"ACCESS_KEY\" : \"XXX\" , \"QUEUE_URL\" : \"https://sqs.us-west-2.amazonaws.com/xxxx/DTHS3Stack-S3TransferQueue-xxxx\" , \"REGION_NAME\" : \"us-west-2\" , \"SECRET_KEY\" : \"XXXXX\" } 4. Click OK .","title":"How to achieve real-time data transfer by OSS event trigger "},{"location":"tutorial-oss/#create-the-trigger","text":"Click the Create Trigger in Triggers tab to create the trigger for the function. Choose OSS as the Trigger Type , choose the bucket name. For Trigger Event , choose: oss:ObjectCreated:PutObject oss:ObjectCreated:PostObject oss:ObjectCreated:CopyObject oss:ObjectCreated:CompleteMultipartUpload oss:ObjectCreated:AppendObject Click OK .","title":"Create the trigger"},{"location":"tutorial-s3/","text":"You can use the web console to create an Amazon S3 transfer task. For more information about how to launch the web console, see deployment . Note Data Transfer Hub also supports using AWS CLI to create an Amazon S3 transfer task. For details, refer to this tutorial . From the Create Transfer Task page, select Create New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source, for example, Amazon S3 . Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . If the data source bucket is also in the account deployed by the solution, please select Yes . If you need to achieve real-time incremental data synchronization, please configure whether to enable S3 event notification. Note that this option can only be configured when the program and your data source are deployed in the same area of the same account. If you do not enable S3 event notification, the program will periodically synchronize incremental data according to the scheduling frequency you configure in the future. If the source bucket is not in the same account where Data Transfer Hub was deployed, select No , then specify the credentials for the source bucket. If you choose to synchronize objects with multiple prefixes, please transfer the prefix list file separated by rows to the root directory of the data source bucket, and then fill in the name of the file. For details, please refer to Multi-Prefix List Configuration Tutorial \u3002 To create credential information, select Secrets Manager to jump to the AWS Secrets Manager console in the current region. From the left menu, select Secrets , then choose Store a new secret and select the other type of secrets key type. Fill in the access_key_id and secret_access_key information in the Plaintext input box according to the following format. For more information, refer to IAM features in the IAM User Guide . Choose Next . { \"access_key_id\" : \"<Your Access Key ID>\" , \"secret_access_key\" : \"<Your Access Key Secret>\" } (Optional) Enter the key name and description. Choose Next . In the configuration of automatic rotation, select Disable automatic rotation. Choose Next . Keep the default value and choose Save to complete the creation of the key. Navigate back to the Data Transfer Hub task creation interface and refresh the interface. Your new secret is displayed in the drop-down list. Select the certificate (Secret). Provide destination settings for the S3 buckets. Note If the source S3 bucket is in the same account where Data Transfer Hub was deployed, then in destination settings, you must create or provide credential information for the S3 destination bucket. Otherwise, no credential information is needed. Use the following steps to update the destination settings. From Engine settings , verify the values and modify them if necessary. We recommend to have the minimum capacity set to at least 1 if for incremental data transfer. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . From Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. If you want to skip the data comparison process and transfer all files, please select No . If you only want to synchronize files with differences, please select Yes . Enter an email address in Alarm Email . Choose Next and review your task parameter details. Choose Create Task . After the task is created successfully, it will appear on the Tasks page. How to transfer S3 object from KMS encrypted Amazon S3 By default, Data Transfer Hub supports data source bucket using SSE-S3 and SSE-KMS. If your source bucket enabled SSE-CMK , you need to create an IAM Policy and attach it to DTH worker and finder node. You can go to Amazon IAM Roles Console and search for <StackName>-FinderStackFinderRole<random suffix> and <StackName>-EC2WorkerStackWorkerAsgRole<random suffix> . Pay attention to the following: Change the Resource in kms part to your own KMS key's arn. For S3 buckets in AWS China Regions, make sure to use arn:aws-cn:kms::: instead of arn:aws:kms::: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\", \"kms:Encrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": [ \"arn:aws:kms:us-west-2:123456789012:key/f5cd8cb7-476c-4322-ac9b-0c94a687700d <Please replace this with your own KMS key arn>\" ] } ] }","title":"Create Amazon S3 transfer task"},{"location":"tutorial-s3/#how-to-transfer-s3-object-from-kms-encrypted-amazon-s3","text":"By default, Data Transfer Hub supports data source bucket using SSE-S3 and SSE-KMS. If your source bucket enabled SSE-CMK , you need to create an IAM Policy and attach it to DTH worker and finder node. You can go to Amazon IAM Roles Console and search for <StackName>-FinderStackFinderRole<random suffix> and <StackName>-EC2WorkerStackWorkerAsgRole<random suffix> . Pay attention to the following: Change the Resource in kms part to your own KMS key's arn. For S3 buckets in AWS China Regions, make sure to use arn:aws-cn:kms::: instead of arn:aws:kms::: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\", \"kms:Encrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": [ \"arn:aws:kms:us-west-2:123456789012:key/f5cd8cb7-476c-4322-ac9b-0c94a687700d <Please replace this with your own KMS key arn>\" ] } ] }","title":"How to transfer S3 object from KMS encrypted Amazon S3"},{"location":"uninstall/","text":"You can uninstall the Data Transfer Hub solution from the AWS Management Console or by using the AWS Command Line Interface. Note Before uninstalling the solution, you must manually stop any active data transfer tasks. Using the AWS Management Console Sign in to the AWS CloudFormation console. On the Stacks page, select this solution\u2019s installation stack. Choose Delete . Using AWS Command Line Interface Determine whether the AWS Command Line Interface (AWS CLI) is available in your environment. For installation instructions, refer to What Is the AWS Command Line Interface in the AWS CLI User Guide . After confirming that the AWS CLI is available, run the following command. $ aws cloudformation delete-stack --stack-name <installation-stack-name> Deleting the Amazon S3 buckets This solution is configured to retain the solution-created Amazon S3 bucket (for deploying in an opt-in Region) if you decide to delete the AWS CloudFormation stack to prevent accidental data loss. After uninstalling the solution, you can manually delete this S3 bucket if you do not need to retain the data. Follow these steps to delete the Amazon S3 bucket. Sign in to the Amazon S3 console Choose Buckets from the left navigation pane. Locate the S3 buckets. Select the S3 bucket and choose Delete . To delete the S3 bucket using AWS CLI, run the following command: $ aws s3 rb s3://<bucket-name> --force","title":"Uninstall the solution"},{"location":"uninstall/#using-the-aws-management-console","text":"Sign in to the AWS CloudFormation console. On the Stacks page, select this solution\u2019s installation stack. Choose Delete .","title":"Using the AWS Management Console"},{"location":"uninstall/#using-aws-command-line-interface","text":"Determine whether the AWS Command Line Interface (AWS CLI) is available in your environment. For installation instructions, refer to What Is the AWS Command Line Interface in the AWS CLI User Guide . After confirming that the AWS CLI is available, run the following command. $ aws cloudformation delete-stack --stack-name <installation-stack-name>","title":"Using AWS Command Line Interface"},{"location":"uninstall/#deleting-the-amazon-s3-buckets","text":"This solution is configured to retain the solution-created Amazon S3 bucket (for deploying in an opt-in Region) if you decide to delete the AWS CloudFormation stack to prevent accidental data loss. After uninstalling the solution, you can manually delete this S3 bucket if you do not need to retain the data. Follow these steps to delete the Amazon S3 bucket. Sign in to the Amazon S3 console Choose Buckets from the left navigation pane. Locate the S3 buckets. Select the S3 bucket and choose Delete . To delete the S3 bucket using AWS CLI, run the following command: $ aws s3 rb s3://<bucket-name> --force","title":"Deleting the Amazon S3 buckets"},{"location":"upgrade/","text":"Upgrade Data Transfer Hub Time to upgrade : Approximately 20 minutes Upgrade Overview Use the following steps to upgrade the solution on AWS console. Step 1. Update the CloudFormation Stack Step 2. (Optional) Update the OIDC configuration Step 3. Create an invalidation on CloudFront Step 4. Refresh the web console Step 1. Update the CloudFormation stack Go to the AWS CloudFormation console . Select the Data Transfer Hub main stack, and click the Update button. Choose Replace current template , and enter the specific Amazon S3 URL according to your initial deployment type. Refer to Deployment Overview for more details. Type Link Launch in Global Regions https://aws-gcr-solutions.s3.amazonaws.com/data-transfer-hub/latest/DataTransferHub-cognito.template Launch in China Regions https://aws-gcr-solutions.s3.cn-north-1.amazonaws.com.cn/data-transfer-hub/latest/DataTransferHub-openid.template Under Parameters , review the parameters for the template and modify them as necessary. Choose Next . On Configure stack options page, choose Next . On Review page, review and confirm the settings. Check the box I acknowledge that AWS CloudFormation might create IAM resources . Choose Update stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a UPDATE_COMPLETE status in approximately 15 minutes. Step 2. (Optional) Update the OIDC configuration If you have deployed the solution in China Region with OIDC, refer to the deployment section to update the authorization and authentication configuration in OIDC. Step 3. Create an invalidation on CloudFront CloudFront has cached an old version of Data Transfer Hub console at its pop locations. We need to create an invalidation on the CloudFront console to force the deletion of cache. Go to the AWS CloudFront console . Choose the Distribution of Data Transfer Hub. The Description is like SolutionName - Web Console Distribution (RegionName) . On the Invalidation page, click Create invalidation , and create an invalidation with /* . Step 4. Refresh the web console Now you have completed all the upgrade steps. Please click the refresh button in your browser.","title":"Upgrade the solution"},{"location":"upgrade/#upgrade-data-transfer-hub","text":"Time to upgrade : Approximately 20 minutes","title":"Upgrade Data Transfer Hub"},{"location":"upgrade/#upgrade-overview","text":"Use the following steps to upgrade the solution on AWS console. Step 1. Update the CloudFormation Stack Step 2. (Optional) Update the OIDC configuration Step 3. Create an invalidation on CloudFront Step 4. Refresh the web console","title":"Upgrade Overview"},{"location":"upgrade/#step-1-update-the-cloudformation-stack","text":"Go to the AWS CloudFormation console . Select the Data Transfer Hub main stack, and click the Update button. Choose Replace current template , and enter the specific Amazon S3 URL according to your initial deployment type. Refer to Deployment Overview for more details. Type Link Launch in Global Regions https://aws-gcr-solutions.s3.amazonaws.com/data-transfer-hub/latest/DataTransferHub-cognito.template Launch in China Regions https://aws-gcr-solutions.s3.cn-north-1.amazonaws.com.cn/data-transfer-hub/latest/DataTransferHub-openid.template Under Parameters , review the parameters for the template and modify them as necessary. Choose Next . On Configure stack options page, choose Next . On Review page, review and confirm the settings. Check the box I acknowledge that AWS CloudFormation might create IAM resources . Choose Update stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a UPDATE_COMPLETE status in approximately 15 minutes.","title":"Step 1. Update the CloudFormation stack"},{"location":"upgrade/#step-2-optional-update-the-oidc-configuration","text":"If you have deployed the solution in China Region with OIDC, refer to the deployment section to update the authorization and authentication configuration in OIDC.","title":"Step 2. (Optional) Update the OIDC configuration "},{"location":"upgrade/#step-3-create-an-invalidation-on-cloudfront","text":"CloudFront has cached an old version of Data Transfer Hub console at its pop locations. We need to create an invalidation on the CloudFront console to force the deletion of cache. Go to the AWS CloudFront console . Choose the Distribution of Data Transfer Hub. The Description is like SolutionName - Web Console Distribution (RegionName) . On the Invalidation page, click Create invalidation , and create an invalidation with /* .","title":"Step 3. Create an invalidation on CloudFront"},{"location":"upgrade/#step-4-refresh-the-web-console","text":"Now you have completed all the upgrade steps. Please click the refresh button in your browser.","title":"Step 4. Refresh the web console"}]}