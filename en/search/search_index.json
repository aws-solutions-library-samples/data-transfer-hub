{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Data Transfer Hub solution provides secure, scalable, and trackable data transfer for Amazon Simple Storage Service (Amazon S3) objects and Amazon Elastic Container Registry (Amazon ECR) images. This data transfer helps customers expand their businesses globally by easily moving data in and out of Amazon Web Services (AWS) China Regions. This implementation guide provides an overview of the Data Transfer Hub solution, its reference architecture and components, considerations for planning the deployment, configuration steps for deploying the Data Transfer Hub solution to the AWS Cloud. Use this navigation table to quickly find answers to these questions: If you want to \u2026 Read\u2026 Know the cost for running this solution Cost Understand the security considerations for this solution Security Know how to plan for quotas for this solution Quotas Know which AWS Regions are supported for this solution Supported AWS Regions View or download the AWS CloudFormation template included in this solution to automatically deploy the infrastructure resources (the \u201cstack\u201d) for this solution AWS CloudFormation templates This guide is intended for IT architects, developers, DevOps, data analysts, and marketing technology professionals who have practical experience architecting in the AWS Cloud. You will be responsible for your compliance with all applicable laws in respect of your data transfer tasks.","title":"Overview"},{"location":"contributors/","text":"Aiden Dai Eva Liu Kervin Hu Haiyun Chen Joe Shi Ashwini Rudra Jyoti Tyagi","title":"Contributors"},{"location":"faq/","text":"The following are common questions you might have when deploying and using the solution. Deployment 1. In which AWS Regions can this solution be deployed? For the list of supported Regions, refer to supported Regions . 2. When creating a transfer task, should I deploy it on the data source side or the destination side? The transfer performance of the solution will not be affected by whether the deployment is on the data source or destination side. If you do not have a domain name registered by ICP in AWS China Regions, we recommend you deploy it in AWS Regions. If you need to deploy in AWS China Regions but do not have a domain name, you can directly deploy the back-end version: Amazon S3 Plugin: https://github.com/awslabs/data-transfer-hub/blob/main/docs/S3_PLUGIN.md Amazon ECR Plugin: https://github.com/awslabs/data-transfer-hub/blob/main/docs/ECR_PLUGIN.md 3. Do I need to deploy the solution on the data source and destination side separately? No. You can choose to deploy on the data source or destination side, which has no impact on the transfer performance. 4. Is it possible to deploy the solution in AWS account A and transfer Amazon S3 objects from account B to account C? Yes. In this case, you need to store the AccessKeyID and SecretAccessKey of account B and account C in the Secrets Manager of account A. 5. For data transfer within the production account, is it recommended to create an AWS account specifically for deploying the solution? Yes. It is recommended to create a new AWS account dedicated to deploying solutions. The account-level isolation improves the stability of the production account in the data synchronization process. 6. Is it possible to transfer data between different areas under the same account? Not supported currently. For this scenario, we recommend using Amazon S3's Cross-Region Replication . 7. Can I use AWS CLI to create a DTH S3 Transfer Task? Yes. Please refer to the tutorial Using AWS CLI to launch DTH S3 Transfer task . Performance 1. Will there be any difference in data transfer performance for deployment in AWS China Regions and in AWS Regions? No. If you do not have a domain name registered by ICP in AWS China Regions, it is recommended to deploy it in the AWS Regions. 2. What are the factors influencing the data transfer performance? The transfer performance may be affected by average file size, destination of data transfer, geographic location of data source, and real-time network environment. For example, using the same configuration, the transfer speed with an average file size of 50MB is 170 times the transfer speed with an average file size of 10KB. 3. What is the scale up/scale down policy of Worker Auto Scaling Group? The Auto Scaling Group will automatically scale up or scale down according to the number of tasks in SQS. Scaling Up Steps are: { lower : 100 , cha n ge : + 1 } { lower : 500 , cha n ge : + 2 } { lower : 2000 , cha n ge : + 5 } { lower : 10000 , cha n ge : + 10 } Scaling Down Step is: { upper : 0 , cha n ge : -10000 } Data security and authentication 1. How does the solution ensure data security? The solution adopts the following to ensure data security: All data is transferred in the memory in the transfer node cluster, without being placed on the disk. The external ports of all transfer nodes are closed, and there is no way to SSH into the transfer node. All data download and upload bottom layers are calling AWS official API, and data transfer conforms to the TLS protocol . 2. How does the solution ensure the security of resources on the cloud? In the research and development process, we strictly follow the minimum IAM permission design rules, and adopt the design of Auto Scaling, which will automatically help users terminate idle working nodes. 3. Is the front-end console open to the public network? How to ensure user authentication and multi-user management? Yes. You can access it with a front-end console link. User authentication and multi-user management are achieved through AWS Cognito User Pool in AWS Regions, and through OIDC SAAS in AWS China Regions. 4. How does the solution achieve cross-account and cross-cloud authentication? By authentication through the Access Keyid and Access Key of the other party\u2019s account. The secret key is stored in AWS Secrets Manager and will be read in Secrets Manager as needed. 5. Does the solution support SSE-S3, SSE-KMS, and SSE-CMK? Yes. The solution supports the use of SSE-S3 and SSE-KMS data sources. If your source bucket has SSE-CMK enabled, refer to the tutorial . Features 1. What third-party clouds does Amazon S3 sync currently support? Alibaba Cloud OSS, Tencent Cloud, Huawei Cloud, Qiniu Cloud, Baidu Cloud, and all clouds that support S3 compatible protocols. 2. Why is the status of Task still in progress after all destination files are transferred? When will the task stop? For Fixed Rate Job The data difference between the data source and destination will be monitored continuously, and the differences between the two sides will be automatically compared after the first deployment. Moreover, when the default comparison task once an hour finds a difference, it will also transfer the difference data. Therefore, the status of the Task will always be in progress, unless the user manually terminates the task. Based on the built-in automatic expansion function of the solution, when there is no data to be transferred, the number of transfer working nodes will be automatically reduced to the minimum value configured by the user. For One Time Transfer Job When the objects are all transferred to the destination, the status of one time transfer job will become Completed . The transfer action will stop and you can select Stop to delete and release all backend resources. 3. How often will the data difference between the data source and destination be compared\uff1f By default, it runs hourly. At Task Scheduling Settings , you can make the task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . 4. Is it possible for real-time synchronization of newly added files? Near-real-time synchronization can be achieved, only if the Data Transfer Hub is deployed in the same AWS account and the same region as the data source. If the data source and the solution are not in the same account, you can configure it manually. For more information, refer to the tutorial . 5. Are there restrictions on the number of files and the size of files? No. Larger files will be uploaded in chunks. 6. If a single file transfer fails due to network issues, how to resolve it? Is there an error handling mechanism? There will be 5 retries. After 5 retries without success, the task will be notified to the user via email. 7. How to monitor the progress of the transfer by checking information like how many files are waiting to be transferred and the current transfer speed? You can jump to the customized dashboard of Amazon CloudWatch by clicking the CloudWatch Dashboard link in Task Detail of the web console. You can also go directly to CloudWatch to view it. 8. Do I need to create an S3 destination bucket before creating a transfer task? Yes, you need to create the destination S3 bucket in advance. 9. How to use Finder depth and Finder number to improve Finder performance? You can use these two parameters to increase the parallelism of Finder to improve the performance of data comparison. For example, if there are 12 subdirectories with over 100k files each, such as Jan , Feb , ..., Dec . You are recommended to set finderDepth =1 and finderNumber =12, so that your comparison performance will increase by 12 times. When using finderDepth and finderNumber, make sure that there are no objects in the folder whose level is equal to or less than finderDepth. Otherwise, data loss may occur. For example, assume that you set the finderDepth =2 and finderNumber =12 * 31 = 372, and your S3 bucket structure is like bucket_name/Jan/01/pic1.jpg . What will be lost are files like bucket_name/pic.jpg , bucket_name/Jan/pic.jpg . What will not be lost are all files under bucket_name/Jan/33/ , all files under bucket_name/13/33/ . 10. How to deal with Access Key Rotation? Currently, when Data Transfer Hub perceived that the Access Key of S3 has been rotated, it will fetch the latest key from AWS Secrets Manager automatically. Therefore, the Access Key Rotation will not affect the migrating process of DTH. 11. Does the Payer Request mode support Public Data Set? No. Currently, Payer Request data synchronization is only supported through Access Key and Private Key authentication methods. Others 1. The cluster node (EC2) is terminated by mistake. How to resolve it? The Auto Scaling mechanism of the solution will enable automatic restart of a new working node. However, if a sharding task being transferred in the node is mistakenly terminated, it may cause that the files to which the shard belongs cannot be merged on the destination side, and the error \"api error NoSuchUpload: The specified upload does not exist. The upload ID may be invalid, or the upload may have been aborted or completed\" occurs. You need to configure lifecycle rules for Delete expired delete markers or incomplete multipart uploads in the Amazon S3 bucket. 2. The Secrets configuration in Secrets Manager is wrong. How to resolve it? You need to update Secrets in Secrets Manager first, and then go to the EC2 console to Terminate all EC2 instances that have been started by the task. Later, the Auto Scaling mechanism of the solution will automatically start a new working node and update Secrets to it. 3. How to find detailed transfer log? For Portal users Go to Tasks list page, and click the Task ID . You can see the dashboard and logs under the Monitoring section. Data Transfer Hub has embedded Dashboard and log groups on the Portal, so you do not need to navigate to AWS CloudWatch console to view the logs. For Plugin (Pure Backend) users When deploying the stack, you will be asked to enter the stack name ( DTHS3Stack by default), and most resources will be created with the name prefix as the stack name. For example, the format of the queue name is <StackName>-S3TransferQueue-<random suffix> . This plugin will create two main log groups. If there is no data transfer, you need to check whether there is a problem in the Finder task log. The following is the log group for scheduling Finder tasks. For more information, refer to the Troubleshooting section. <StackName>-CommonS3RepWorkerLogGroup<random suffix> The following are the log groups of all EC2 instances, and you can find detailed transfer logs. <StackName>-EC2WorkerStackS3RepWorkerLogGroup<random suffix> 4. How to make customized build? If you want to make customized changes to this plugin, refer to Custom Build . 5. After the deployment is complete, why can't I find any log streams in the two CloudWatch log groups? This is because the subnet you selected when deploying this solution does not have public network access, and the EC2 cannot download the CloudWatch agent to send logs to CloudWatch. Check your VPC settings. After resolving the issue, you need to manually terminate the running EC2 instance (if any) through this solution. Later, the elastic scaling group will automatically start a new instance. 6. How to use TLSv1.2_2021 or above for this Solution? Please go to the CloudFront Console and configure a custom domain, which will allow you to select a Security policy for CloudFront after solution deployment. You need to prepare a domain name and a corresponding TLS certificate in order to use more secure TLS configurations.","title":"FAQ"},{"location":"faq/#deployment","text":"1. In which AWS Regions can this solution be deployed? For the list of supported Regions, refer to supported Regions . 2. When creating a transfer task, should I deploy it on the data source side or the destination side? The transfer performance of the solution will not be affected by whether the deployment is on the data source or destination side. If you do not have a domain name registered by ICP in AWS China Regions, we recommend you deploy it in AWS Regions. If you need to deploy in AWS China Regions but do not have a domain name, you can directly deploy the back-end version: Amazon S3 Plugin: https://github.com/awslabs/data-transfer-hub/blob/main/docs/S3_PLUGIN.md Amazon ECR Plugin: https://github.com/awslabs/data-transfer-hub/blob/main/docs/ECR_PLUGIN.md 3. Do I need to deploy the solution on the data source and destination side separately? No. You can choose to deploy on the data source or destination side, which has no impact on the transfer performance. 4. Is it possible to deploy the solution in AWS account A and transfer Amazon S3 objects from account B to account C? Yes. In this case, you need to store the AccessKeyID and SecretAccessKey of account B and account C in the Secrets Manager of account A. 5. For data transfer within the production account, is it recommended to create an AWS account specifically for deploying the solution? Yes. It is recommended to create a new AWS account dedicated to deploying solutions. The account-level isolation improves the stability of the production account in the data synchronization process. 6. Is it possible to transfer data between different areas under the same account? Not supported currently. For this scenario, we recommend using Amazon S3's Cross-Region Replication . 7. Can I use AWS CLI to create a DTH S3 Transfer Task? Yes. Please refer to the tutorial Using AWS CLI to launch DTH S3 Transfer task .","title":"Deployment"},{"location":"faq/#performance","text":"1. Will there be any difference in data transfer performance for deployment in AWS China Regions and in AWS Regions? No. If you do not have a domain name registered by ICP in AWS China Regions, it is recommended to deploy it in the AWS Regions. 2. What are the factors influencing the data transfer performance? The transfer performance may be affected by average file size, destination of data transfer, geographic location of data source, and real-time network environment. For example, using the same configuration, the transfer speed with an average file size of 50MB is 170 times the transfer speed with an average file size of 10KB. 3. What is the scale up/scale down policy of Worker Auto Scaling Group? The Auto Scaling Group will automatically scale up or scale down according to the number of tasks in SQS. Scaling Up Steps are: { lower : 100 , cha n ge : + 1 } { lower : 500 , cha n ge : + 2 } { lower : 2000 , cha n ge : + 5 } { lower : 10000 , cha n ge : + 10 } Scaling Down Step is: { upper : 0 , cha n ge : -10000 }","title":"Performance"},{"location":"faq/#data-security-and-authentication","text":"1. How does the solution ensure data security? The solution adopts the following to ensure data security: All data is transferred in the memory in the transfer node cluster, without being placed on the disk. The external ports of all transfer nodes are closed, and there is no way to SSH into the transfer node. All data download and upload bottom layers are calling AWS official API, and data transfer conforms to the TLS protocol . 2. How does the solution ensure the security of resources on the cloud? In the research and development process, we strictly follow the minimum IAM permission design rules, and adopt the design of Auto Scaling, which will automatically help users terminate idle working nodes. 3. Is the front-end console open to the public network? How to ensure user authentication and multi-user management? Yes. You can access it with a front-end console link. User authentication and multi-user management are achieved through AWS Cognito User Pool in AWS Regions, and through OIDC SAAS in AWS China Regions. 4. How does the solution achieve cross-account and cross-cloud authentication? By authentication through the Access Keyid and Access Key of the other party\u2019s account. The secret key is stored in AWS Secrets Manager and will be read in Secrets Manager as needed. 5. Does the solution support SSE-S3, SSE-KMS, and SSE-CMK? Yes. The solution supports the use of SSE-S3 and SSE-KMS data sources. If your source bucket has SSE-CMK enabled, refer to the tutorial .","title":"Data security and authentication"},{"location":"faq/#features","text":"1. What third-party clouds does Amazon S3 sync currently support? Alibaba Cloud OSS, Tencent Cloud, Huawei Cloud, Qiniu Cloud, Baidu Cloud, and all clouds that support S3 compatible protocols. 2. Why is the status of Task still in progress after all destination files are transferred? When will the task stop? For Fixed Rate Job The data difference between the data source and destination will be monitored continuously, and the differences between the two sides will be automatically compared after the first deployment. Moreover, when the default comparison task once an hour finds a difference, it will also transfer the difference data. Therefore, the status of the Task will always be in progress, unless the user manually terminates the task. Based on the built-in automatic expansion function of the solution, when there is no data to be transferred, the number of transfer working nodes will be automatically reduced to the minimum value configured by the user. For One Time Transfer Job When the objects are all transferred to the destination, the status of one time transfer job will become Completed . The transfer action will stop and you can select Stop to delete and release all backend resources. 3. How often will the data difference between the data source and destination be compared\uff1f By default, it runs hourly. At Task Scheduling Settings , you can make the task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . 4. Is it possible for real-time synchronization of newly added files? Near-real-time synchronization can be achieved, only if the Data Transfer Hub is deployed in the same AWS account and the same region as the data source. If the data source and the solution are not in the same account, you can configure it manually. For more information, refer to the tutorial . 5. Are there restrictions on the number of files and the size of files? No. Larger files will be uploaded in chunks. 6. If a single file transfer fails due to network issues, how to resolve it? Is there an error handling mechanism? There will be 5 retries. After 5 retries without success, the task will be notified to the user via email. 7. How to monitor the progress of the transfer by checking information like how many files are waiting to be transferred and the current transfer speed? You can jump to the customized dashboard of Amazon CloudWatch by clicking the CloudWatch Dashboard link in Task Detail of the web console. You can also go directly to CloudWatch to view it. 8. Do I need to create an S3 destination bucket before creating a transfer task? Yes, you need to create the destination S3 bucket in advance. 9. How to use Finder depth and Finder number to improve Finder performance? You can use these two parameters to increase the parallelism of Finder to improve the performance of data comparison. For example, if there are 12 subdirectories with over 100k files each, such as Jan , Feb , ..., Dec . You are recommended to set finderDepth =1 and finderNumber =12, so that your comparison performance will increase by 12 times. When using finderDepth and finderNumber, make sure that there are no objects in the folder whose level is equal to or less than finderDepth. Otherwise, data loss may occur. For example, assume that you set the finderDepth =2 and finderNumber =12 * 31 = 372, and your S3 bucket structure is like bucket_name/Jan/01/pic1.jpg . What will be lost are files like bucket_name/pic.jpg , bucket_name/Jan/pic.jpg . What will not be lost are all files under bucket_name/Jan/33/ , all files under bucket_name/13/33/ . 10. How to deal with Access Key Rotation? Currently, when Data Transfer Hub perceived that the Access Key of S3 has been rotated, it will fetch the latest key from AWS Secrets Manager automatically. Therefore, the Access Key Rotation will not affect the migrating process of DTH. 11. Does the Payer Request mode support Public Data Set? No. Currently, Payer Request data synchronization is only supported through Access Key and Private Key authentication methods.","title":"Features"},{"location":"faq/#others","text":"1. The cluster node (EC2) is terminated by mistake. How to resolve it? The Auto Scaling mechanism of the solution will enable automatic restart of a new working node. However, if a sharding task being transferred in the node is mistakenly terminated, it may cause that the files to which the shard belongs cannot be merged on the destination side, and the error \"api error NoSuchUpload: The specified upload does not exist. The upload ID may be invalid, or the upload may have been aborted or completed\" occurs. You need to configure lifecycle rules for Delete expired delete markers or incomplete multipart uploads in the Amazon S3 bucket. 2. The Secrets configuration in Secrets Manager is wrong. How to resolve it? You need to update Secrets in Secrets Manager first, and then go to the EC2 console to Terminate all EC2 instances that have been started by the task. Later, the Auto Scaling mechanism of the solution will automatically start a new working node and update Secrets to it. 3. How to find detailed transfer log? For Portal users Go to Tasks list page, and click the Task ID . You can see the dashboard and logs under the Monitoring section. Data Transfer Hub has embedded Dashboard and log groups on the Portal, so you do not need to navigate to AWS CloudWatch console to view the logs. For Plugin (Pure Backend) users When deploying the stack, you will be asked to enter the stack name ( DTHS3Stack by default), and most resources will be created with the name prefix as the stack name. For example, the format of the queue name is <StackName>-S3TransferQueue-<random suffix> . This plugin will create two main log groups. If there is no data transfer, you need to check whether there is a problem in the Finder task log. The following is the log group for scheduling Finder tasks. For more information, refer to the Troubleshooting section. <StackName>-CommonS3RepWorkerLogGroup<random suffix> The following are the log groups of all EC2 instances, and you can find detailed transfer logs. <StackName>-EC2WorkerStackS3RepWorkerLogGroup<random suffix> 4. How to make customized build? If you want to make customized changes to this plugin, refer to Custom Build . 5. After the deployment is complete, why can't I find any log streams in the two CloudWatch log groups? This is because the subnet you selected when deploying this solution does not have public network access, and the EC2 cannot download the CloudWatch agent to send logs to CloudWatch. Check your VPC settings. After resolving the issue, you need to manually terminate the running EC2 instance (if any) through this solution. Later, the elastic scaling group will automatically start a new instance. 6. How to use TLSv1.2_2021 or above for this Solution? Please go to the CloudFront Console and configure a custom domain, which will allow you to select a Security policy for CloudFront after solution deployment. You need to prepare a domain name and a corresponding TLS certificate in order to use more secure TLS configurations.","title":"Others"},{"location":"notices/","text":"Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents Amazon Web Services current product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from Amazon Web Services and its affiliates, suppliers or licensors. Amazon Web Services products or services are provided \u201cas is\u201d without warranties, representations, or conditions of any kind, whether express or implied. Amazon Web Services responsibilities and liabilities to its customers are controlled by Amazon Web Services agreements, and this document is not part of, nor does it modify, any agreement between Amazon Web Services and its customers. Data Transfer Hub is licensed under the terms of the of the Apache License Version 2.0 available at Classless Inter-Domain Routing (CIDR) .","title":"Notices"},{"location":"revisions/","text":"Date Description January 2021 Initial release of version 1.0 July 2021 Released version 2.0 1. Support general OIDC providers, including Authing, Auth0, okta, etc. 2. Support transferring objects from more Amazon S3 compatible storage services, such as Huawei Cloud OBS. 3. Support setting the access control list (ACL) of the target bucket object 4. Support deployment in account A, and copying data from account B to account C 5. Change to use Graviton 2 instance, and turn on BBR to transfer S3 objects to improve performance and save costs 6. Change to use Secrets Manager to maintain credential information December 2021 Released version 2.1 1. Support custom prefix list to filter transfer tasks 2. Support configuration of single-run file transfer tasks 3. Support configuration of tasks through custom CRON Expression timetable 4. Support manual enabling or disabling of data comparison function July 2022 Released version 2.2 1. Support transfer data through Direct Connect March 2023 Released version 2.3 1. Support embedded dashboard and logs 2. Support S3 Access Key Rotation 3. Enhance One Time Transfer Task monitoring April 2023 Released version 2.4 1. Support payer request S3 object transfer September 2023 Released version 2.5 1. Added support for transferring ECR assets without tags 2. Optimize stop task operation, add new filter condition to view all history tasks 3. Enhanced transfer performance by utilizing cluster capabilities through parallel multipart upload for large file transfers 4.Added automatic restart functionality for the Worker CLI 5.Enabled IMDSv2 by default for Auto Scaling Groups","title":"Revisions"},{"location":"troubleshooting/","text":"After creating the task, you may encounter some error messages. The following list the error messages and provide general steps to troubleshoot them. 1. StatusCode: 400, InvalidToken: The provided token is malformed or otherwise invalid If you get this error message, confirm that your secret is configured in the following format. You can copy and paste it directly. { \"access_key_id\" : \"<Your Access Key ID>\" , \"secret_access_key\" : \"<Your Access Key Secret>\" } 2. StatusCode: 403, InvalidAccessKeyId: The AWS Access Key Id you provided does not exist in our records If you get this error message, check if your bucket name and region name are configured correctly. 3. StatusCode: 403, InvalidAccessKeyId: UnknownError If you get this error message, check whether the Credential stored in Secrets Manager has the proper permissions. For more information, refer to IAM Policy . 4. StatusCode: 400, AccessDeniedException: Access to KMS is not allowed If you get this error message, confirm that your secret is not encrypted by SSE-CMK. Currently, DTH does not support SSE-CMK encrypted secrets. 5. dial tcp: lookup xxx.xxxxx.xxxxx.xx (http://xxx.xxxxx.xxxxx.xx/) on xxx.xxx.xxx.xxx:53: no such host If you get this error message, check if your endpoint is configured correctly.","title":"Troubleshooting"},{"location":"uninstall/","text":"You can uninstall the Data Transfer Hub solution from the AWS Management Console or by using the AWS Command Line Interface. You must manually stop any active transfer tasks before uninstalling. Using the AWS Management Console Sign in to the AWS CloudFormation console. On the Stacks page, select this solution\u2019s installation stack. Choose Delete . Using AWS Command Line Interface Determine whether the AWS Command Line Interface (AWS CLI) is available in your environment. For installation instructions, refer to What Is the AWS Command Line Interface in the AWS CLI User Guide . After confirming that the AWS CLI is available, run the following command. $ aws cloudformation delete-stack --stack-name <installation-stack-name> Deleting the Amazon S3 buckets This solution is configured to retain the solution-created Amazon S3 bucket (for deploying in an opt-in Region) if you decide to delete the AWS CloudFormation stack to prevent accidental data loss. After uninstalling the solution, you can manually delete this S3 bucket if you do not need to retain the data. Follow these steps to delete the Amazon S3 bucket. Sign in to the Amazon S3 console. Choose Buckets from the left navigation pane. Locate the <stack-name> S3 buckets. Select the S3 bucket and choose Delete . To delete the S3 bucket using AWS CLI, run the following command: $ aws s3 rb s3://<bucket-name> --force","title":"Uninstall the solution"},{"location":"uninstall/#using-the-aws-management-console","text":"Sign in to the AWS CloudFormation console. On the Stacks page, select this solution\u2019s installation stack. Choose Delete .","title":"Using the AWS Management Console"},{"location":"uninstall/#using-aws-command-line-interface","text":"Determine whether the AWS Command Line Interface (AWS CLI) is available in your environment. For installation instructions, refer to What Is the AWS Command Line Interface in the AWS CLI User Guide . After confirming that the AWS CLI is available, run the following command. $ aws cloudformation delete-stack --stack-name <installation-stack-name>","title":"Using AWS Command Line Interface"},{"location":"uninstall/#deleting-the-amazon-s3-buckets","text":"This solution is configured to retain the solution-created Amazon S3 bucket (for deploying in an opt-in Region) if you decide to delete the AWS CloudFormation stack to prevent accidental data loss. After uninstalling the solution, you can manually delete this S3 bucket if you do not need to retain the data. Follow these steps to delete the Amazon S3 bucket. Sign in to the Amazon S3 console. Choose Buckets from the left navigation pane. Locate the <stack-name> S3 buckets. Select the S3 bucket and choose Delete . To delete the S3 bucket using AWS CLI, run the following command: $ aws s3 rb s3://<bucket-name> --force","title":"Deleting the Amazon S3 buckets"},{"location":"update/","text":"Time to upgrade : Approximately 20 minutes Upgrade Overview Use the following steps to upgrade the solution on AWS console. Step 1. Update the CloudFormation Stack Step 2. (Optional) Update the OIDC configuration Step 3. Create an invalidation on CloudFront Step 4. Refresh the web console Step 1. Update the CloudFormation stack Go to the AWS CloudFormation console . Select the Data Transfer Hub main stack, and click the Update button. Choose Replace current template , and enter the specific Amazon S3 URL according to your initial deployment type. Refer to Deployment Overview for more details. Type Link Launch in Global Regions https://s3.amazonaws.com/solutions-reference/data-transfer-hub/latest/DataTransferHub-cognito.template Launch in China Regions https://s3.amazonaws.com/solutions-reference/data-transfer-hub/latest/DataTransferHub-openid.template Under Parameters , review the parameters for the template and modify them as necessary. Choose Next . On Configure stack options page, choose Next . On Review page, review and confirm the settings. Check the box I acknowledge that AWS CloudFormation might create IAM resources . Choose Update stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a UPDATE_COMPLETE status in approximately 15 minutes. Step 2. (Optional) Update the OIDC configuration If you have deployed the solution in China Region with OIDC, refer to the deployment section to update the authorization and authentication configuration in OIDC. Step 3. Refresh the web console Now you have completed all the upgrade steps. Please click the refresh button in your browser.","title":"Upgrade the solution"},{"location":"update/#upgrade-overview","text":"Use the following steps to upgrade the solution on AWS console. Step 1. Update the CloudFormation Stack Step 2. (Optional) Update the OIDC configuration Step 3. Create an invalidation on CloudFront Step 4. Refresh the web console","title":"Upgrade Overview"},{"location":"update/#step-1-update-the-cloudformation-stack","text":"Go to the AWS CloudFormation console . Select the Data Transfer Hub main stack, and click the Update button. Choose Replace current template , and enter the specific Amazon S3 URL according to your initial deployment type. Refer to Deployment Overview for more details. Type Link Launch in Global Regions https://s3.amazonaws.com/solutions-reference/data-transfer-hub/latest/DataTransferHub-cognito.template Launch in China Regions https://s3.amazonaws.com/solutions-reference/data-transfer-hub/latest/DataTransferHub-openid.template Under Parameters , review the parameters for the template and modify them as necessary. Choose Next . On Configure stack options page, choose Next . On Review page, review and confirm the settings. Check the box I acknowledge that AWS CloudFormation might create IAM resources . Choose Update stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a UPDATE_COMPLETE status in approximately 15 minutes.","title":"Step 1. Update the CloudFormation stack"},{"location":"update/#step-2-optional-update-the-oidc-configuration","text":"If you have deployed the solution in China Region with OIDC, refer to the deployment section to update the authorization and authentication configuration in OIDC.","title":"Step 2. (Optional) Update the OIDC configuration "},{"location":"update/#step-3-refresh-the-web-console","text":"Now you have completed all the upgrade steps. Please click the refresh button in your browser.","title":"Step 3. Refresh the web console"},{"location":"architecture-overview/architecture-details/","text":"This section describes the components and AWS services that make up this solution and the architecture details on how these components work together. AWS services in this solution The following AWS services are included in this solution: AWS service Description Amazon CloudFront Core . To made available the static web assets (frontend user interface). AWS AppSync Core . To provide the backend APIs. AWS Lambda Core . To call backend APIs. Amazon ECS Core . To run the container images used by the plugin template. Amazon DynamoDB Core . To store a record with transfer status for each object. Amazon EC2 Core . To consume the messages in Amazon SQS and transfer the object from the source bucket to the destination bucket. AWS Secrets Manager Core . Stores the credential for data transfer. AWS Step Functions Supporting . To start or stop/delete the ECR or S3 plugin template. Amazon S3 Supporting . To store the static web assets (frontend user interface). Amazon Cognito Supporting . To authenticate users (in AWS Regions). Amazon ECR Supporting . To host the container images. Amazon SQS Supporting . To store the transfer tasks temporarily as a buffer. Amazon EventBridge Supporting . To invoke the transfer tasks regularly. Amazon SNS Supporting . Provides topic and email subscription notifications for data transfer results. AWS CloudWatch Supporting . To monitor the data transfer progress. How Data Transfer Hub works This solution has three components: a web console, the Amazon S3 transfer engine, and the Amazon ECR transfer engine. Web console This solution provides a simple web console which allows you to create and manage transfer tasks for Amazon S3 and Amazon ECR. Amazon S3 transfer engine Amazon S3 transfer engine runs the Amazon S3 plugin and is used for transferring objects from their sources into S3 buckets. The S3 plugin supports the following features: Transfer Amazon S3 objects between AWS China Regions and AWS Regions Transfer objects from Alibaba Cloud OSS / Tencent COS / Qiniu Kodo to Amazon S3 Transfer objects from S3 Compatible Storage service to Amazon S3 Support near real time transfer via S3 Event Support transfer with object metadata Support incremental data transfer Support transfer from private payer request bucket Auto retry and error handling Amazon ECR transfer engine Amazon ECR engine runs the Amazon ECR plugin and is used for transferring container images from other container registries. The ECR plugin supports the following features: Transfer Amazon ECR images between AWS China Regions and AWS Regions Transfer from public container registry (such as Docker Hub, GCR.io, Quay.io) to Amazon ECR Transfer selected images to Amazon ECR Transfer all images and tags from Amazon ECR The ECR plugin leverages skopeo for the underlying engine. The AWS Lambda function lists images in their sources and uses Fargate to run the transfer jobs.","title":"Architecture details"},{"location":"architecture-overview/architecture-details/#aws-services-in-this-solution","text":"The following AWS services are included in this solution: AWS service Description Amazon CloudFront Core . To made available the static web assets (frontend user interface). AWS AppSync Core . To provide the backend APIs. AWS Lambda Core . To call backend APIs. Amazon ECS Core . To run the container images used by the plugin template. Amazon DynamoDB Core . To store a record with transfer status for each object. Amazon EC2 Core . To consume the messages in Amazon SQS and transfer the object from the source bucket to the destination bucket. AWS Secrets Manager Core . Stores the credential for data transfer. AWS Step Functions Supporting . To start or stop/delete the ECR or S3 plugin template. Amazon S3 Supporting . To store the static web assets (frontend user interface). Amazon Cognito Supporting . To authenticate users (in AWS Regions). Amazon ECR Supporting . To host the container images. Amazon SQS Supporting . To store the transfer tasks temporarily as a buffer. Amazon EventBridge Supporting . To invoke the transfer tasks regularly. Amazon SNS Supporting . Provides topic and email subscription notifications for data transfer results. AWS CloudWatch Supporting . To monitor the data transfer progress.","title":"AWS services in this solution"},{"location":"architecture-overview/architecture-details/#how-data-transfer-hub-works","text":"This solution has three components: a web console, the Amazon S3 transfer engine, and the Amazon ECR transfer engine.","title":"How Data Transfer Hub works"},{"location":"architecture-overview/architecture-details/#web-console","text":"This solution provides a simple web console which allows you to create and manage transfer tasks for Amazon S3 and Amazon ECR.","title":"Web console"},{"location":"architecture-overview/architecture-details/#amazon-s3-transfer-engine","text":"Amazon S3 transfer engine runs the Amazon S3 plugin and is used for transferring objects from their sources into S3 buckets. The S3 plugin supports the following features: Transfer Amazon S3 objects between AWS China Regions and AWS Regions Transfer objects from Alibaba Cloud OSS / Tencent COS / Qiniu Kodo to Amazon S3 Transfer objects from S3 Compatible Storage service to Amazon S3 Support near real time transfer via S3 Event Support transfer with object metadata Support incremental data transfer Support transfer from private payer request bucket Auto retry and error handling","title":"Amazon S3 transfer engine"},{"location":"architecture-overview/architecture-details/#amazon-ecr-transfer-engine","text":"Amazon ECR engine runs the Amazon ECR plugin and is used for transferring container images from other container registries. The ECR plugin supports the following features: Transfer Amazon ECR images between AWS China Regions and AWS Regions Transfer from public container registry (such as Docker Hub, GCR.io, Quay.io) to Amazon ECR Transfer selected images to Amazon ECR Transfer all images and tags from Amazon ECR The ECR plugin leverages skopeo for the underlying engine. The AWS Lambda function lists images in their sources and uses Fargate to run the transfer jobs.","title":"Amazon ECR transfer engine"},{"location":"architecture-overview/architecture/","text":"Deploying this solution with the default parameters builds the following environment in the AWS Cloud. Data Transfer Hub architecture This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings. The solution\u2019s static web assets (frontend user interface) are stored in Amazon S3 and made available through Amazon CloudFront . The backend APIs are provided via AWS AppSync GraphQL. Users are authenticated by either Amazon Cognito User Pool (in AWS Regions) or by an OpenID connect provider (in AWS China Regions) such as Authing , Auth0 , etc. AWS AppSync runs AWS Lambda to call backend APIs. Lambda starts an AWS Step Functions workflow that uses AWS CloudFormation to start or stop/delete the Amazon ECR or Amazon S3 plugin template. The plugin templates are hosted in a centralized Amazon S3 bucket managed by AWS. The solution also provisions an Amazon ECS cluster that runs the container images used by the plugin template, and the container images are hosted in Amazon ECR . The data transfer task information is stored in Amazon DynamoDB . After deploying the solution, you can use AWS WAF to protect CloudFront or AppSync. Important If you deploy this solution in AWS (Beijing) Region operated by Beijing Sinnet Technology Co., Ltd. (Sinnet), or the AWS (Ningxia) Region operated by Ningxia Western Cloud Data Technology Co., Ltd. ( ), you are required to provide a domain with ICP Recordal before you can access the web console. The web console is a centralized place to create and manage all data transfer jobs. Each data type (for example, Amazon S3 or Amazon ECR) is a plugin for Data Transfer Hub, and is packaged as an AWS CloudFormation template hosted in an S3 bucket that AWS owns. When the you create a transfer task, an AWS Lambda function initiates the Amazon CloudFormation template, and state of each task is stored and displayed in the DynamoDB tables. As of this revision, the solution supports two data transfer plugins: an Amazon S3 plugin and an Amazon ECR plugin. Amazon S3 plugin Data Transfer Hub Amazon S3 plugin architecture The Amazon S3 plugin runs the following workflows: A time-based Event Bridge rule triggers a AWS Lambda function on an hourly basis. AWS Lambda uses the launch template to launch a data comparison job (JobFinder) in an Amazon Elastic Compute Cloud (Amazon EC2) . The job lists all the objects in the source and destination buckets, makes comparisons among objects and determines which objects should be transferred. Amazon EC2 sends a message for each object that will be transferred to Amazon Simple Queue Service (Amazon SQS) . Amazon S3 event messages can also be supported for more real-time data transfer; whenever there is object uploaded to source bucket, the event message is sent to the same Amazon SQS queue. A JobWorker running in Amazon EC2 consumes the messages in SQS and transfers the object from the source bucket to the destination bucket. You can use an Auto Scaling group to control the number of EC2 instances to transfer the data based on business need. A record with transfer status for each object is stored in Amazon DynamoDB. The Amazon EC2 instance will get (download) the object from the source bucket based on the Amazon SQS message. The Amazon EC2 instance will put (upload) the object to the destination bucket based on the Amazon SQS message. When the JobWorker node identifies a large file (with a default threshold of 1 GB) for the first time, a Multipart Upload task running in Amazon EC2 is initiated. The corresponding UploadId is then conveyed to the AWS Step Functions, which invokes a scheduled recurring task. Every minute, AWS Step Functions verifies the successful transmission of the distributed shards associated with the UploadId across the entire cluster. If all shards have been transmitted successfully, Amazon EC2 invokes the CompleteMultipartUpload API in Amazon S3 to finalize the consolidation of the shards. Otherwise, any invalid shards are discarded. Note If an object (or part of an object) transfer failed, the JobWorker releases the message in the queue, and the object is transferred again after the message is visible in the queue (default visibility timeout is set to 15 minutes). If the transfer failed five times, the message is sent to the dead letter queue and a notification alarm is initiated. Amazon ECR plugin Data Transfer Hub Amazon ECR plugin architecture The Amazon ECR plugin runs the following workflows: An EventBridge rule runs an AWS Step Functions workflow on a regular basis (by default, it runs daily). Step Functions invokes AWS Lambda to retrieve the list of images from the source. Lambda will either list all the repository content in the source Amazon ECR, or get the stored image list from System Manager Parameter Store. The transfer task will run within Fargate in a maximum concurrency of 10. If a transfer task failed for some reason, it will automatically retry three times. Each task uses skopeo to copy the images into the target ECR. After the copy completes, the status (either success or fail) is logged into DynamoDB for tracking purpose.","title":"Architecture diagram"},{"location":"architecture-overview/architecture/#amazon-s3-plugin","text":"Data Transfer Hub Amazon S3 plugin architecture The Amazon S3 plugin runs the following workflows: A time-based Event Bridge rule triggers a AWS Lambda function on an hourly basis. AWS Lambda uses the launch template to launch a data comparison job (JobFinder) in an Amazon Elastic Compute Cloud (Amazon EC2) . The job lists all the objects in the source and destination buckets, makes comparisons among objects and determines which objects should be transferred. Amazon EC2 sends a message for each object that will be transferred to Amazon Simple Queue Service (Amazon SQS) . Amazon S3 event messages can also be supported for more real-time data transfer; whenever there is object uploaded to source bucket, the event message is sent to the same Amazon SQS queue. A JobWorker running in Amazon EC2 consumes the messages in SQS and transfers the object from the source bucket to the destination bucket. You can use an Auto Scaling group to control the number of EC2 instances to transfer the data based on business need. A record with transfer status for each object is stored in Amazon DynamoDB. The Amazon EC2 instance will get (download) the object from the source bucket based on the Amazon SQS message. The Amazon EC2 instance will put (upload) the object to the destination bucket based on the Amazon SQS message. When the JobWorker node identifies a large file (with a default threshold of 1 GB) for the first time, a Multipart Upload task running in Amazon EC2 is initiated. The corresponding UploadId is then conveyed to the AWS Step Functions, which invokes a scheduled recurring task. Every minute, AWS Step Functions verifies the successful transmission of the distributed shards associated with the UploadId across the entire cluster. If all shards have been transmitted successfully, Amazon EC2 invokes the CompleteMultipartUpload API in Amazon S3 to finalize the consolidation of the shards. Otherwise, any invalid shards are discarded. Note If an object (or part of an object) transfer failed, the JobWorker releases the message in the queue, and the object is transferred again after the message is visible in the queue (default visibility timeout is set to 15 minutes). If the transfer failed five times, the message is sent to the dead letter queue and a notification alarm is initiated.","title":"Amazon S3 plugin"},{"location":"architecture-overview/architecture/#amazon-ecr-plugin","text":"Data Transfer Hub Amazon ECR plugin architecture The Amazon ECR plugin runs the following workflows: An EventBridge rule runs an AWS Step Functions workflow on a regular basis (by default, it runs daily). Step Functions invokes AWS Lambda to retrieve the list of images from the source. Lambda will either list all the repository content in the source Amazon ECR, or get the stored image list from System Manager Parameter Store. The transfer task will run within Fargate in a maximum concurrency of 10. If a transfer task failed for some reason, it will automatically retry three times. Each task uses skopeo to copy the images into the target ECR. After the copy completes, the status (either success or fail) is logged into DynamoDB for tracking purpose.","title":"Amazon ECR plugin"},{"location":"architecture-overview/design-considerations/","text":"This solution was designed with best practices from the AWS Well-Architected Framework which helps customers design and operate reliable, secure, efficient, and cost-effective workloads in the cloud. This section describes how the design principles and best practices of the Well-Architected Framework were applied when building this solution. Operational excellence This section describes how the principles and best practices of the operational excellence pillar were applied when designing this solution. The Data Transfer Hub solution pushes metrics to Amazon CloudWatch at various stages to provide observability into the infrastructure, Lambda functions, Amazon EC2 transfer workers, Step Function workflow and the rest of the solution components. Data transferring errors are added to the Amazon SQS queue for retries and alerts. Security This section describes how the principles and best practices of the security pillar were applied when designing this solution. Data Transfer Hub web console users are authenticated and authorized with Amazon Cognito. All inter-service communications use AWS IAM roles. All roles used by the solution follows least-privilege access. That is, it only contains minimum permissions required so the service can function properly. Reliability This section describes how the principles and best practices of the reliability pillar were applied when designing this solution. Using AWS serverless services wherever possible (for example, Lambda, Step Functions, Amazon S3, and Amazon SQS) to ensure high availability and recovery from service failure. Data is stored in DynamoDB and Amazon S3, so it persists in multiple Availability Zones (AZs) by default. Performance efficiency This section describes how the principles and best practices of the performance efficiency pillar were applied when designing this solution. The ability to launch this solution in any Region that supports AWS services in this solution such as: AWS Lambda, AWS S3, Amazon SQS, Amazon DynamoDB, and Amazon EC2. Automatically testing and deploying this solution daily. Reviewing this solution by solution architects and subject matter experts for areas to experiment and improve. Cost optimization This section describes how the principles and best practices of the cost optimization pillar were applied when designing this solution. Use Autoscaling Group so that the compute costs are only related to how much data is transferred. Using serverless services such as Amazon SQS and DynamoDB so that customers only get charged for what they use. Sustainability This section describes how the principles and best practices of the sustainability pillar were applied when designing this solution. The solution\u2018s serverless design (using Lambda, Amazon SQS and DynamoDB) and the use of managed services (such as Amazon EC2) are aimed at reducing carbon footprint compared to the footprint of continually operating on-premises servers.","title":"AWS Well-Architected pillars"},{"location":"architecture-overview/design-considerations/#operational-excellence","text":"This section describes how the principles and best practices of the operational excellence pillar were applied when designing this solution. The Data Transfer Hub solution pushes metrics to Amazon CloudWatch at various stages to provide observability into the infrastructure, Lambda functions, Amazon EC2 transfer workers, Step Function workflow and the rest of the solution components. Data transferring errors are added to the Amazon SQS queue for retries and alerts.","title":"Operational excellence"},{"location":"architecture-overview/design-considerations/#security","text":"This section describes how the principles and best practices of the security pillar were applied when designing this solution. Data Transfer Hub web console users are authenticated and authorized with Amazon Cognito. All inter-service communications use AWS IAM roles. All roles used by the solution follows least-privilege access. That is, it only contains minimum permissions required so the service can function properly.","title":"Security"},{"location":"architecture-overview/design-considerations/#reliability","text":"This section describes how the principles and best practices of the reliability pillar were applied when designing this solution. Using AWS serverless services wherever possible (for example, Lambda, Step Functions, Amazon S3, and Amazon SQS) to ensure high availability and recovery from service failure. Data is stored in DynamoDB and Amazon S3, so it persists in multiple Availability Zones (AZs) by default.","title":"Reliability"},{"location":"architecture-overview/design-considerations/#performance-efficiency","text":"This section describes how the principles and best practices of the performance efficiency pillar were applied when designing this solution. The ability to launch this solution in any Region that supports AWS services in this solution such as: AWS Lambda, AWS S3, Amazon SQS, Amazon DynamoDB, and Amazon EC2. Automatically testing and deploying this solution daily. Reviewing this solution by solution architects and subject matter experts for areas to experiment and improve.","title":"Performance efficiency"},{"location":"architecture-overview/design-considerations/#cost-optimization","text":"This section describes how the principles and best practices of the cost optimization pillar were applied when designing this solution. Use Autoscaling Group so that the compute costs are only related to how much data is transferred. Using serverless services such as Amazon SQS and DynamoDB so that customers only get charged for what they use.","title":"Cost optimization"},{"location":"architecture-overview/design-considerations/#sustainability","text":"This section describes how the principles and best practices of the sustainability pillar were applied when designing this solution. The solution\u2018s serverless design (using Lambda, Amazon SQS and DynamoDB) and the use of managed services (such as Amazon EC2) are aimed at reducing carbon footprint compared to the footprint of continually operating on-premises servers.","title":"Sustainability"},{"location":"deployment/deployment-overview/","text":"Use the following steps to deploy this solution on AWS. For detailed instructions, follow the links for each step. Before you launch the solution, review the cost , architecture, network security, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account. Time to deploy : Approximately 15 minutes Step 1. Launch the stack (Option 1) Deploy the AWS CloudFormation template in AWS Regions (Option 2) Deploy the AWS CloudFormation template in AWS China Regions Step 2. Launch the web console Step 3. Create a transfer task","title":"Deployment overview"},{"location":"deployment/deployment/","text":"Step 1. (Option 1) Launch the stack in AWS Regions Important The following deployment instructions apply to AWS Regions only. For deployment in AWS China Regions, refer to Option 2. Deploy the AWS CloudFormation template for Option 1 \u2013 AWS Regions Note You are responsible for the cost of the AWS services used while running this solution. For more details, visit the Cost section in this guide, and refer to the pricing webpage for each AWS service used in this solution. Sign in to the AWS Management Console and select the button to launch the DataTransferHub-cognito.template AWS CloudFormation template. Alternatively, you can download the template as a starting point for your own implementation. The template launches in the US East (N. Virginia) Region by default. To launch the solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and and AWS STS quotas in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for this solution template and modify them as necessary. This solution uses the following default values. Parameter Default Description AdminEmail <Requires input> The email of the Admin user. Choose Next . On the Configure Stack Options page, keep the default values and choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. Step 1. (Option 2) Launch the stack in AWS China Regions Important The following deployment instructions apply to AWS China Regions only. For deployment in AWS Regions, refer to Option 1. Prerequisites Create an OIDC user pool. Configure domain name service (DNS) resolution. Make sure a domain registered by ICP is available. Prerequisite 1: Create an OIDC user pool In AWS Regions where Amazon Cognito is not yet available, you can use OIDC to provide authentication. The following procedure uses AWS Partner Authing as an example, but you can also choose any available provider. Go to the Authing console . Create a new user pool if you don't have one. Select the user pool. On the left navigation bar, select Self-built App under Applications . Click the Create button. Enter the Application Name , and Subdomain . Save the App ID (that is, client_id ) and Issuer to a text file from Endpoint Information, which will be used later. Update the Login Callback URL and Logout Callback URL to your IPC recorded domain name. Set the Authorization Configuration. Update login control. Select and enter the Application interface from the left sidebar, select Login Control , and then select Registration and Login . Please select only Password Login: Email for the login method. Please uncheck all options in the registration method. Select Save . Create an admin user. From Users & Roles , select Users , then choose Create user . Enter the email for the user. Choose OK . Check the email for a temporary password. Reset the user password. Note Because this solution does not support application roles, all the users will receive admin rights. Prerequisite 2: Configure domain name service resolution Configure domain name service (DNS) resolution to point the ICP licensed domain to the CloudFront default domain name. Optionally, you can use your own DNS resolver. The following is an example for configuring an Amazon Route 53. Create a hosted zone in Amazon Route 53. For more information, refer to the Amazon Route 53 Developer Guide . Create a CNAME record for the console URL. From the hosted zone, choose Create Record . In the Record name input box, enter the host name. From Record type select CNAME . In the value field, enter the CloudFormation output PortalUrl. Select Create records . Add alternative domain names to the CloudFront distribution. Configure the corresponding domain name in CloudFront to open the CloudFront console by finding the distribution ID for PortalURL in the list and selecting ID (or check the check box, and then select Distribution Settings ). Click Edit , and add the record of Route 53 in the previous step to the Alternate Domain Name (CNAME) . Deploy the AWS CloudFormation template for Option 2 \u2013 AWS China Regions This automated AWS CloudFormation template deploys Data Transfer Hub in the AWS Cloud. You must Create an ODIC User Pool and Configure DNS resolution before launching the stack. Note You are responsible for the cost of the AWS services used while running this solution. For more details, visit the Cost section in this guide, and refer to the pricing webpage for each AWS service used in this solution. Sign in to the AWS Management Console and select the button to launch the DataTransferHub-openid.template AWS CloudFormation template. Alternatively, you can download the template as a starting point for your own implementation. The template launches in your console\u2019s default Region. To launch the solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and AWS STS quotas in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for this solution template and modify them as necessary. This solution uses the following default values. Parameter Default Description OidcProvider <Requires input> Refers to the Issuer shown in the OIDC application configuration. OidcClientId <Requires input> Refers to the App ID shown in the OIDC application configuration. OidcCustomerDomain <Requires input> Refers to the customer domain that has completed ICP registration in China, not the subdomain provided by Authing. It must start with https:// . AdminEmail <Requires input> Refers to the email for receiving task status alarm. Choose Next . On the Configure Stack Options page, keep the default values and choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create Stack to deploy the stack. You can view the status of your stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. Step 2. Launch the web console After the stack is successfully created, navigate to the CloudFormation Outputs tab and select the PortalUrl value to access the Data Transfer Hub web console. After successful deployment, an email containing the temporary login password will be sent to the email address provided. Depending on the region where you start the stack, you can choose to access the web console from the AWS China Regions or the AWS Regions. Log in with Amazon Cognito User Pool (for AWS Regions) Log in with OpenID using Authing.cn (for AWS China Regions) (Option 1) Log in using Amazon Cognito user pool for AWS Regions Using a web browser, enter the PortalURL from the CloudFormation Output tab, then navigate to the Amazon Cognito console. Sign in with the AdminEmail and the temporary password. Set a new account password. (Optional) Verify your email address for account recovery. After the verification is complete, the system opens the Data Transfer Hub web console. (Option 2) OpenID authentication for AWS China Regions Using a web browser, enter the Data Transfer Hub domain name. If you are logging in for the first time, the system will open the Authing.cn login interface. Enter the username and password you registered when you deployed the solution, then choose Login. The system opens the Data Transfer Hub web console. Change your password and then sign in again. Step 3. Create a transfer task Use the web console to create a transfer task for Amazon S3 or Amazon ECR. For more information, refer to Create Amazon S3 Transfer Task and Create Amazon ECR Transfer Task . Data Transfer Hub web console","title":"Deployment"},{"location":"deployment/deployment/#step-1-option-1-launch-the-stack-in-aws-regions","text":"Important The following deployment instructions apply to AWS Regions only. For deployment in AWS China Regions, refer to Option 2. Deploy the AWS CloudFormation template for Option 1 \u2013 AWS Regions Note You are responsible for the cost of the AWS services used while running this solution. For more details, visit the Cost section in this guide, and refer to the pricing webpage for each AWS service used in this solution. Sign in to the AWS Management Console and select the button to launch the DataTransferHub-cognito.template AWS CloudFormation template. Alternatively, you can download the template as a starting point for your own implementation. The template launches in the US East (N. Virginia) Region by default. To launch the solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and and AWS STS quotas in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for this solution template and modify them as necessary. This solution uses the following default values. Parameter Default Description AdminEmail <Requires input> The email of the Admin user. Choose Next . On the Configure Stack Options page, keep the default values and choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Step 1. (Option 1) Launch the stack in AWS Regions "},{"location":"deployment/deployment/#step-1-option-2-launch-the-stack-in-aws-china-regions","text":"Important The following deployment instructions apply to AWS China Regions only. For deployment in AWS Regions, refer to Option 1.","title":"Step 1. (Option 2) Launch the stack in AWS China Regions "},{"location":"deployment/deployment/#prerequisites","text":"Create an OIDC user pool. Configure domain name service (DNS) resolution. Make sure a domain registered by ICP is available.","title":"Prerequisites"},{"location":"deployment/deployment/#prerequisite-1-create-an-oidc-user-pool","text":"In AWS Regions where Amazon Cognito is not yet available, you can use OIDC to provide authentication. The following procedure uses AWS Partner Authing as an example, but you can also choose any available provider. Go to the Authing console . Create a new user pool if you don't have one. Select the user pool. On the left navigation bar, select Self-built App under Applications . Click the Create button. Enter the Application Name , and Subdomain . Save the App ID (that is, client_id ) and Issuer to a text file from Endpoint Information, which will be used later. Update the Login Callback URL and Logout Callback URL to your IPC recorded domain name. Set the Authorization Configuration. Update login control. Select and enter the Application interface from the left sidebar, select Login Control , and then select Registration and Login . Please select only Password Login: Email for the login method. Please uncheck all options in the registration method. Select Save . Create an admin user. From Users & Roles , select Users , then choose Create user . Enter the email for the user. Choose OK . Check the email for a temporary password. Reset the user password. Note Because this solution does not support application roles, all the users will receive admin rights.","title":"Prerequisite 1: Create an OIDC user pool"},{"location":"deployment/deployment/#prerequisite-2-configure-domain-name-service-resolution","text":"Configure domain name service (DNS) resolution to point the ICP licensed domain to the CloudFront default domain name. Optionally, you can use your own DNS resolver. The following is an example for configuring an Amazon Route 53. Create a hosted zone in Amazon Route 53. For more information, refer to the Amazon Route 53 Developer Guide . Create a CNAME record for the console URL. From the hosted zone, choose Create Record . In the Record name input box, enter the host name. From Record type select CNAME . In the value field, enter the CloudFormation output PortalUrl. Select Create records . Add alternative domain names to the CloudFront distribution. Configure the corresponding domain name in CloudFront to open the CloudFront console by finding the distribution ID for PortalURL in the list and selecting ID (or check the check box, and then select Distribution Settings ). Click Edit , and add the record of Route 53 in the previous step to the Alternate Domain Name (CNAME) . Deploy the AWS CloudFormation template for Option 2 \u2013 AWS China Regions This automated AWS CloudFormation template deploys Data Transfer Hub in the AWS Cloud. You must Create an ODIC User Pool and Configure DNS resolution before launching the stack. Note You are responsible for the cost of the AWS services used while running this solution. For more details, visit the Cost section in this guide, and refer to the pricing webpage for each AWS service used in this solution. Sign in to the AWS Management Console and select the button to launch the DataTransferHub-openid.template AWS CloudFormation template. Alternatively, you can download the template as a starting point for your own implementation. The template launches in your console\u2019s default Region. To launch the solution in a different AWS Region, use the Region selector in the console navigation bar. On the Create stack page, verify that the correct template URL is in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and AWS STS quotas in the AWS Identity and Access Management User Guide . Under Parameters , review the parameters for this solution template and modify them as necessary. This solution uses the following default values. Parameter Default Description OidcProvider <Requires input> Refers to the Issuer shown in the OIDC application configuration. OidcClientId <Requires input> Refers to the App ID shown in the OIDC application configuration. OidcCustomerDomain <Requires input> Refers to the customer domain that has completed ICP registration in China, not the subdomain provided by Authing. It must start with https:// . AdminEmail <Requires input> Refers to the email for receiving task status alarm. Choose Next . On the Configure Stack Options page, keep the default values and choose Next . On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create Stack to deploy the stack. You can view the status of your stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.","title":"Prerequisite 2: Configure domain name service resolution"},{"location":"deployment/deployment/#step-2-launch-the-web-console","text":"After the stack is successfully created, navigate to the CloudFormation Outputs tab and select the PortalUrl value to access the Data Transfer Hub web console. After successful deployment, an email containing the temporary login password will be sent to the email address provided. Depending on the region where you start the stack, you can choose to access the web console from the AWS China Regions or the AWS Regions. Log in with Amazon Cognito User Pool (for AWS Regions) Log in with OpenID using Authing.cn (for AWS China Regions)","title":"Step 2. Launch the web console "},{"location":"deployment/deployment/#option-1-log-in-using-amazon-cognito-user-pool-for-aws-regions","text":"Using a web browser, enter the PortalURL from the CloudFormation Output tab, then navigate to the Amazon Cognito console. Sign in with the AdminEmail and the temporary password. Set a new account password. (Optional) Verify your email address for account recovery. After the verification is complete, the system opens the Data Transfer Hub web console.","title":"(Option 1) Log in using Amazon Cognito user pool for AWS Regions "},{"location":"deployment/deployment/#option-2-openid-authentication-for-aws-china-regions","text":"Using a web browser, enter the Data Transfer Hub domain name. If you are logging in for the first time, the system will open the Authing.cn login interface. Enter the username and password you registered when you deployed the solution, then choose Login. The system opens the Data Transfer Hub web console. Change your password and then sign in again.","title":"(Option 2) OpenID authentication for AWS China Regions "},{"location":"deployment/deployment/#step-3-create-a-transfer-task","text":"Use the web console to create a transfer task for Amazon S3 or Amazon ECR. For more information, refer to Create Amazon S3 Transfer Task and Create Amazon ECR Transfer Task . Data Transfer Hub web console","title":"Step 3. Create a transfer task "},{"location":"deployment/template/","text":"To automate deployment, this solution uses the following AWS CloudFormation templates, which you can download before deployment: DataTransferHub-cognito.template : Use this template to launch the solution and all associated components in AWS Regions where Amazon Cognito is available. The default configuration deploys Amazon S3, Amazon CloudFront, AWS AppSync, Amazon DynamoDB, AWS Lambda, Amazon ECS, and Amazon Cognito, but you can customize the template to meet your specific needs. DataTransferHub-openid.template : Use this template to launch the solution and all associated components in AWS China Regions where Amazon Cognito is not available. The default configuration deploys Amazon S3, Amazon CloudFront, AWS AppSync, Amazon DynamoDB, AWS Lambda, and Amazon ECS, but you can customize the template to meet your specific needs.","title":"AWS CloudFormation template"},{"location":"developer-guide/source/","text":"Visit our GitHub repository to download the source files for this solution and to share your customizations with others. The Data Transfer Hub templates are generated using the AWS Cloud Development Kit (AWS CDK). Refer to the README.md file for additional information.","title":"Source code"},{"location":"plan-deployment/cost/","text":"You are responsible for the cost of the AWS services used while running this solution, which can vary based on whether you are transferring Amazon S3 objects or Amazon ECR images. The solution automatically deploys an additional Amazon CloudFront Distribution and an Amazon S3 bucket for storing the static website assets in your account. You are responsible for the incurred variable charges from these services. For full details, refer to the pricing webpage for each AWS service you will be using in this solution. The following examples demonstrate how to estimate the cost. Two example estimates are for transferring Amazon S3 objects, and one is for transferring ECR images. Cost of an Amazon S3 transfer task For an Amazon S3 transfer task, the cost can vary based on the total number of files and the average file size. Example 1 Transfer 1 TB of S3 files from AWS Oregon Region (us-west-2) to AWS Beijing Region (cn-north-1), the average file size is 50MB . Total files: ~20,480 Average speed per EC2 instance: ~1GB/min Total EC2 instance hours: ~17 hours As of this revision, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost Amazon EC2 $0.0084 per hour (t4g.micro) $0.14 Amazon S3 ~ 12 GET requests + 10 PUT requests per file GET: $0.0004 per 1000 request PUT: $0.005 per 1000 request $1.12 Amazon DynamoDB ~2 write requests per file $1.25 per million write $0.05 Amazon SQS ~2 request per file $0.40 per million request $0.01 Data Transfer Out 0.09 per GB $92.16 Others (For example, CloudWatch, Secrets Manager, etc.) $1 TOTAL $94.48 Example 2 Transfer 1 TB of S3 files from AWS Oregon region (us-west-2) to China Beijing Region (cn-north-1), the average file size is 10KB . Total files: ~107,400,000 Average speed per EC2 instance: ~6MB/min (~10 files per sec) Total EC2 instance hours: ~3000 hours As of this revision, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost Amazon EC2 $0.0084 per hour (t4g.micro) $25.20 Amazon S3 ~ 2 GET requests + 1 PUT request per file GET: $0.0004 per 1000 request PUT: $0.005 per 1000 request $622.34 Amazon DynamoDB ~2 write requests per file $1.25 per million write $268.25 Amazon SQS ~2 requests per file $0.40 per million request $85.92 Data Transfer Out 0.09 per GB $92.16 Others (For example, CloudWatch, Secrets Manager, etc.) $20 TOTAL $1113.87 Cost of an Amazon ECR transfer task For an Amazon ECR transfer task, the cost can vary based on network speed and total size of ECR images. Example 3 Transfer 27 Amazon ECR images (~3 GB in total size) from AWS Ireland Region (eu-west-1) to AWS Beijing Region (cn-north-1). The total runtime is about 6 minutes. As of this revision, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost AWS Lambda $0.0000004 per 100ms $0.000072 (35221.95 ms) AWS Step Functions $0.000025 per State Transition (~ 60 state transitions per run in this case) $0.0015 Fargate $0.04048 per vCPU per hour $0.004445 per GB per hour (0.5 vCPU 1GB Memory) $0.015 (~ 2200s) Data Transfer Out 0.09 per GB $0.27 Others (For example, CloudWatch, Secrets Manager, etc.) $0 TOTAL $0.287","title":"Cost"},{"location":"plan-deployment/cost/#cost-of-an-amazon-s3-transfer-task","text":"For an Amazon S3 transfer task, the cost can vary based on the total number of files and the average file size.","title":"Cost of an Amazon S3 transfer task"},{"location":"plan-deployment/cost/#example-1","text":"Transfer 1 TB of S3 files from AWS Oregon Region (us-west-2) to AWS Beijing Region (cn-north-1), the average file size is 50MB . Total files: ~20,480 Average speed per EC2 instance: ~1GB/min Total EC2 instance hours: ~17 hours As of this revision, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost Amazon EC2 $0.0084 per hour (t4g.micro) $0.14 Amazon S3 ~ 12 GET requests + 10 PUT requests per file GET: $0.0004 per 1000 request PUT: $0.005 per 1000 request $1.12 Amazon DynamoDB ~2 write requests per file $1.25 per million write $0.05 Amazon SQS ~2 request per file $0.40 per million request $0.01 Data Transfer Out 0.09 per GB $92.16 Others (For example, CloudWatch, Secrets Manager, etc.) $1 TOTAL $94.48","title":"Example 1"},{"location":"plan-deployment/cost/#example-2","text":"Transfer 1 TB of S3 files from AWS Oregon region (us-west-2) to China Beijing Region (cn-north-1), the average file size is 10KB . Total files: ~107,400,000 Average speed per EC2 instance: ~6MB/min (~10 files per sec) Total EC2 instance hours: ~3000 hours As of this revision, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost Amazon EC2 $0.0084 per hour (t4g.micro) $25.20 Amazon S3 ~ 2 GET requests + 1 PUT request per file GET: $0.0004 per 1000 request PUT: $0.005 per 1000 request $622.34 Amazon DynamoDB ~2 write requests per file $1.25 per million write $268.25 Amazon SQS ~2 requests per file $0.40 per million request $85.92 Data Transfer Out 0.09 per GB $92.16 Others (For example, CloudWatch, Secrets Manager, etc.) $20 TOTAL $1113.87","title":"Example 2"},{"location":"plan-deployment/cost/#cost-of-an-amazon-ecr-transfer-task","text":"For an Amazon ECR transfer task, the cost can vary based on network speed and total size of ECR images.","title":"Cost of an Amazon ECR transfer task"},{"location":"plan-deployment/cost/#example-3","text":"Transfer 27 Amazon ECR images (~3 GB in total size) from AWS Ireland Region (eu-west-1) to AWS Beijing Region (cn-north-1). The total runtime is about 6 minutes. As of this revision, the cost of using the solution to complete the transfer task is shown in the following table: AWS service Dimensions Total Cost AWS Lambda $0.0000004 per 100ms $0.000072 (35221.95 ms) AWS Step Functions $0.000025 per State Transition (~ 60 state transitions per run in this case) $0.0015 Fargate $0.04048 per vCPU per hour $0.004445 per GB per hour (0.5 vCPU 1GB Memory) $0.015 (~ 2200s) Data Transfer Out 0.09 per GB $0.27 Others (For example, CloudWatch, Secrets Manager, etc.) $0 TOTAL $0.287","title":"Example 3"},{"location":"plan-deployment/quotas/","text":"Quotas for AWS services in this solution Make sure you have sufficient quota for each of the services implemented in this solution . For more information, see AWS service quotas . Choose one of the following links to go to the page for that service. To view the service quotas for all AWS services in the documentation without switching pages, view the information in the Service endpoints and quotas page in the PDF instead. AWS CloudFormation quotas Your AWS account has AWS CloudFormation quotas that you should be aware of when launching the stack in this solution. By understanding these quotas, you can avoid limitation errors that would prevent you from deploying this solution successfully. For more information, refer to AWS CloudFormation quotas in the AWS CloudFormation User Guide .","title":"Quotas"},{"location":"plan-deployment/quotas/#quotas-for-aws-services-in-this-solution","text":"Make sure you have sufficient quota for each of the services implemented in this solution . For more information, see AWS service quotas . Choose one of the following links to go to the page for that service. To view the service quotas for all AWS services in the documentation without switching pages, view the information in the Service endpoints and quotas page in the PDF instead.","title":"Quotas for AWS services in this solution"},{"location":"plan-deployment/quotas/#aws-cloudformation-quotas","text":"Your AWS account has AWS CloudFormation quotas that you should be aware of when launching the stack in this solution. By understanding these quotas, you can avoid limitation errors that would prevent you from deploying this solution successfully. For more information, refer to AWS CloudFormation quotas in the AWS CloudFormation User Guide .","title":"AWS CloudFormation quotas"},{"location":"plan-deployment/regions/","text":"This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List . Supported regions for deployment in AWS Regions Region Name Region ID US East (N. Virginia) us-east-1 US East (Ohio) us-east-2 US West (N. California) us-west-1 US West (Oregon) us-west-2 Asia Pacific (Mumbai) ap-south-1 Asia Pacific (Tokyo) ap-northeast-1 Asia Pacific (Seoul) ap-northeast-2 Asia Pacific (Singapore) ap-southeast-1 Asia Pacific (Sydney) ap-southeast-2 Canada (Central) ca-central-1 Europe (Ireland) eu-west-1 Europe (London) eu-west-2 Europe (Stockholm) eu-north-1 Europe (Frankfurt) eu-central-1 South America (S\u00e3o Paulo) sa-east-1 Supported regions for deployment in AWS China Regions Region Name Region ID China (Beijing) Region Operated by Sinnet cn-north-1 China (Ningxia) Region Operated by NWCD cn-northwest-1","title":"Supported Regions"},{"location":"plan-deployment/regions/#supported-regions-for-deployment-in-aws-regions","text":"Region Name Region ID US East (N. Virginia) us-east-1 US East (Ohio) us-east-2 US West (N. California) us-west-1 US West (Oregon) us-west-2 Asia Pacific (Mumbai) ap-south-1 Asia Pacific (Tokyo) ap-northeast-1 Asia Pacific (Seoul) ap-northeast-2 Asia Pacific (Singapore) ap-southeast-1 Asia Pacific (Sydney) ap-southeast-2 Canada (Central) ca-central-1 Europe (Ireland) eu-west-1 Europe (London) eu-west-2 Europe (Stockholm) eu-north-1 Europe (Frankfurt) eu-central-1 South America (S\u00e3o Paulo) sa-east-1","title":"Supported regions for deployment in AWS Regions"},{"location":"plan-deployment/regions/#supported-regions-for-deployment-in-aws-china-regions","text":"Region Name Region ID China (Beijing) Region Operated by Sinnet cn-north-1 China (Ningxia) Region Operated by NWCD cn-northwest-1","title":"Supported regions for deployment in AWS China Regions"},{"location":"plan-deployment/security/","text":"When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, visit AWS Cloud Security . IAM roles AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions, Amazon API Gateway, and Amazon Cognito access to create regional resources. Amazon CloudFront This solution deploys a web console hosted in an Amazon S3 bucket. To help reduce latency and improve security, this solution includes an Amazon CloudFront distribution with an origin access identity, which is a CloudFront user that provides public access to the solution\u2019s website bucket contents. For more information, refer to Restricting Access to Amazon S3 Content by Using an Origin Access Identity in the Amazon CloudFront Developer Guide .","title":"Security"},{"location":"plan-deployment/security/#iam-roles","text":"AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions, Amazon API Gateway, and Amazon Cognito access to create regional resources.","title":"IAM roles"},{"location":"plan-deployment/security/#amazon-cloudfront","text":"This solution deploys a web console hosted in an Amazon S3 bucket. To help reduce latency and improve security, this solution includes an Amazon CloudFront distribution with an origin access identity, which is a CloudFront user that provides public access to the solution\u2019s website bucket contents. For more information, refer to Restricting Access to Amazon S3 Content by Using an Origin Access Identity in the Amazon CloudFront Developer Guide .","title":"Amazon CloudFront"},{"location":"solution-overview/features-and-benefits/","text":"The solution\u2019s web console provides an interface for managing the following tasks: Transferring Amazon S3 objects between AWS China Regions and AWS Regions Transferring data from other cloud providers\u2019 object storage services (including Alibaba Cloud OSS, Tencent COS, and Qiniu Kodo) to Amazon S3 Transferring objects from Amazon S3 compatible object storage service to Amazon S3 Transferring Amazon ECR images between AWS China Regions and AWS Regions Transferring container images from public container registries (for example, Docker Hub, Google gcr.io, Red Hat Quay.io) to Amazon ECR Note If you need to transfer Amazon S3 objects between AWS Regions, we recommend that you use Cross-Region Replication . If you want to transfer Amazon S3 objects within the same AWS Region, we recommend using Same-Region Replication .","title":"Features and benefits"},{"location":"solution-overview/use-cases/","text":"Today, the China market is one of biggest markets in the world. Many international companies are seeking their success in China, as well as a number of Chinese companies are expanding their businesses globally. One of most important steps of in the business is moving their data. S3 Cross-Region Replication and ECR Cross-Region Replication are popular but customers cannot use them to replicate data into China Regions. With the launch of Data Transfer Hub solution, customers can now create S3 and ECR data transfer tasks between AWS Regions and AWS China Regions in a web portal. Moreover, it supports replicating data from cloud providers to AWS. Data Transfer Hub supports the following use cases: Copy Amazon S3 objects between AWS Regions and AWS China Regions. Copy data from other cloud providers\u2019 object storage services (including Alibaba Cloud OSS, Tencent COS, Qiniu Kodo) to Amazon S3. Transfer Amazon ECR images between AWS Regions and AWS China Regions. Transfer Dockers image from public docker registry (for example, Docker Hub, Google gcr.io, Red Hat Quay.io) to Amazon ECR.","title":"Use cases"},{"location":"user-guide/tutorial-cli-launch/","text":"You can use the AWS CLI to create an Amazon S3 transfer task. Note that if you have deployed the DTH Portal at the same time, the tasks started through the CLI will not appear in the Task List on your Portal. Create an Amazon VPC with two public subnets or two private subnets with NAT gateway . Replace <CLOUDFORMATION_URL> as shown below. https://solutions-reference.s3.amazonaws.com/data-transfer-hub/latest/DataTransferS3Stack.template Go to your terminal and enter the following command. For the parameter details, refer to the Parameters table. aws cloudformation create-stack --stack-name dth-s3-task --template-url CLOUDFORMATION_URL \\ --capabilities CAPABILITY_NAMED_IAM \\ --parameters \\ ParameterKey = alarmEmail,ParameterValue = your_email@example.com \\ ParameterKey = destBucket,ParameterValue = dth-receive-cn-north-1 \\ ParameterKey = destPrefix,ParameterValue = test-prefix \\ ParameterKey = destCredentials,ParameterValue = drh-cn-secret-key \\ ParameterKey = destInCurrentAccount,ParameterValue = false \\ ParameterKey = destRegion,ParameterValue = cn-north-1 \\ ParameterKey = destStorageClass,ParameterValue = STANDARD \\ ParameterKey = srcBucket,ParameterValue = dth-us-west-2 \\ ParameterKey = srcInCurrentAccount,ParameterValue = true \\ ParameterKey = srcCredentials,ParameterValue = \\ ParameterKey = srcRegion,ParameterValue = us-west-2 \\ ParameterKey = srcPrefix,ParameterValue = case1 \\ ParameterKey = srcType,ParameterValue = Amazon_S3 \\ ParameterKey = ec2VpcId,ParameterValue = vpc-040bbab85f0e4e088 \\ ParameterKey = ec2Subnets,ParameterValue = subnet-0d1bf2725ab8e94ee \\\\ ,subnet-06d17b2b3286be40e \\ ParameterKey = finderEc2Memory,ParameterValue = 8 \\ ParameterKey = ec2CronExpression,ParameterValue = \"0/60 * * * ? *\" \\ ParameterKey = includeMetadata,ParameterValue = false \\ ParameterKey = srcEvent,ParameterValue = No \\ ParameterKey = maxCapacity,ParameterValue = 20 \\ ParameterKey = minCapacity,ParameterValue = 1 \\ ParameterKey = desiredCapacity,ParameterValue = 1 Parameters Parameter Name Allowed Value Default Value Description alarmEmail An email to which errors will be sent desiredCapacity 1 Desired capacity for Auto Scaling Group destAcl private public-read public-read-write authenticated-read aws-exec-read bucket-owner-read bucket-owner-full-control bucket-owner-full-control Destination access control list destBucket Destination bucket name destCredentials Secret name in Secrets Manager used to keep AK/SK credentials for destination bucket. Leave it blank if the destination bucket is in the current account destInCurrentAccount true false true Indicates whether the destination bucket is in current account. If not, you should provide a credential with read and write access destPrefix Destination prefix (Optional) destRegion Destination region name destStorageClass STANDARD STANDARD_IA ONEZONE_IA INTELLIGENT_TIERING INTELLIGENT_TIERING Destination storage class, which defaults to INTELLIGENT_TIERING isPayerRequest true false false Indicates whether to enable payer request. If true, it will get object in payer request mode. ec2CronExpression 0/60 * * * ? * Cron expression for EC2 Finder task \"\" for one time transfer. finderEc2Memory 8 16 32 64 128 256 8 GB The amount of memory (in GB) used by the Finder task. ec2Subnets Two public subnets or two private subnets with NAT gateway ec2VpcId VPC ID to run EC2 task, for example, vpc-bef13dc7 finderDepth 0 Depth of sub folders to compare in parallel. 0 means comparing all objects in sequence finderNumber 1 The number of finder threads to run in parallel includeMetadata true false false Indicates whether to add replication of object metadata. If true, there will be additional API calls. maxCapacity 20 Maximum capacity for Auto Scaling Group minCapacity 1 Minimum capacity for Auto Scaling Group srcBucket Source bucket name srcCredentials Secret name in Secrets Manager used to keep AK/SK credentials for Source Bucket. Leave it blank if source bucket is in the current account or source is open data srcEndpoint Source Endpoint URL (Optional). Leave it blank unless you want to provide a custom Endpoint URL srcEvent No Create CreateAndDelete No Whether to enable S3 Event to trigger the replication. Note that S3Event is only applicable if source is in the current account srcInCurrentAccount true false false Indicates whether the source bucket is in the current account. If not, you should provide a credential with read access srcPrefix Source prefix (Optional) srcPrefixsListFile Source prefix list file S3 path (Optional). It supports txt type, for example, my_prefix_list.txt, and the maximum number of lines is 10 millions srcRegion Source region name srcSkipCompare true false false Indicates whether to skip the data comparison in task finding process. If yes, all data in the source will be sent to the destination srcType Amazon_S3 Aliyun_OSS Qiniu_Kodo Tencent_COS Amazon_S3 If you choose to use the Endpoint mode, please select Amazon_S3. workerNumber 1 ~ 10 4 The number of worker threads to run in one worker node/instance. For small files (size < 1MB), you can increase the number of workers to improve the transfer performance.","title":"Use AWS CLI to create S3 transfer task"},{"location":"user-guide/tutorial-cli-launch/#parameters","text":"Parameter Name Allowed Value Default Value Description alarmEmail An email to which errors will be sent desiredCapacity 1 Desired capacity for Auto Scaling Group destAcl private public-read public-read-write authenticated-read aws-exec-read bucket-owner-read bucket-owner-full-control bucket-owner-full-control Destination access control list destBucket Destination bucket name destCredentials Secret name in Secrets Manager used to keep AK/SK credentials for destination bucket. Leave it blank if the destination bucket is in the current account destInCurrentAccount true false true Indicates whether the destination bucket is in current account. If not, you should provide a credential with read and write access destPrefix Destination prefix (Optional) destRegion Destination region name destStorageClass STANDARD STANDARD_IA ONEZONE_IA INTELLIGENT_TIERING INTELLIGENT_TIERING Destination storage class, which defaults to INTELLIGENT_TIERING isPayerRequest true false false Indicates whether to enable payer request. If true, it will get object in payer request mode. ec2CronExpression 0/60 * * * ? * Cron expression for EC2 Finder task \"\" for one time transfer. finderEc2Memory 8 16 32 64 128 256 8 GB The amount of memory (in GB) used by the Finder task. ec2Subnets Two public subnets or two private subnets with NAT gateway ec2VpcId VPC ID to run EC2 task, for example, vpc-bef13dc7 finderDepth 0 Depth of sub folders to compare in parallel. 0 means comparing all objects in sequence finderNumber 1 The number of finder threads to run in parallel includeMetadata true false false Indicates whether to add replication of object metadata. If true, there will be additional API calls. maxCapacity 20 Maximum capacity for Auto Scaling Group minCapacity 1 Minimum capacity for Auto Scaling Group srcBucket Source bucket name srcCredentials Secret name in Secrets Manager used to keep AK/SK credentials for Source Bucket. Leave it blank if source bucket is in the current account or source is open data srcEndpoint Source Endpoint URL (Optional). Leave it blank unless you want to provide a custom Endpoint URL srcEvent No Create CreateAndDelete No Whether to enable S3 Event to trigger the replication. Note that S3Event is only applicable if source is in the current account srcInCurrentAccount true false false Indicates whether the source bucket is in the current account. If not, you should provide a credential with read access srcPrefix Source prefix (Optional) srcPrefixsListFile Source prefix list file S3 path (Optional). It supports txt type, for example, my_prefix_list.txt, and the maximum number of lines is 10 millions srcRegion Source region name srcSkipCompare true false false Indicates whether to skip the data comparison in task finding process. If yes, all data in the source will be sent to the destination srcType Amazon_S3 Aliyun_OSS Qiniu_Kodo Tencent_COS Amazon_S3 If you choose to use the Endpoint mode, please select Amazon_S3. workerNumber 1 ~ 10 4 The number of worker threads to run in one worker node/instance. For small files (size < 1MB), you can increase the number of workers to improve the transfer performance.","title":"Parameters"},{"location":"user-guide/tutorial-directconnect/","text":"This tutorial describes how to use Data Transfer Hub (DTH) via Direct Connect (DX). When the DTH worker node and finder node start to work, they need to download related assets (such as CloudWatch agent, DTH CLI) from the internet by default. In an isolated network, you need to manually download and upload these files to an S3 bucket in the region where DTH is deployed. You have two options to use DTH to transfer data via DX: Use DTH to transfer data via DX in a non-isolated network Use DTH to transfer data via DX in an isolated network Use DTH to transfer data via DX in a non-isolated network In this scenario, DTH is deployed in the destination side and within a VPC with public access (has Internet Gateway or NAT), and the source bucket is in the isolated network. Note As DTH deployment VPC has public internet access (IGW or NAT), EC2 worker/finder nodes can access other AWS services used by DTH such as secret managers and download related assets (such as CloudWatch agent, DTH CLI) from internet without any changes. From the Create Transfer Task page, select Create New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source Amazon S3 Compatible Storage . Enter endpoint url , which must be the interface endpoint url, such as https://bucket.vpce-076205013d3a9a2ca-us23z2ze.s3.ap-east-1.vpce.amazonaws.com . You can find the specific url in VPC Endpoint Console DNS names part. Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . Provide destination settings for the S3 buckets. From Engine settings , verify the values and modify them if necessary. For incremental data transfer, we recommend to set the minimum capacity to at least 1. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . For Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. If you want to skip the data comparison process and transfer all files, select No . If you only want to synchronize files with differences, select Yes . In Alarm Email , provide an email address. Choose Next and review your task parameter details. Choose Create Task . Use DTH to transfer data via DX in an isolated network In this scenario, DTH is deployed in the destination side and within a VPC without public access (isolated VPC), and the source bucket is also in an isolated network. For details, refer to the [tutorial][https://github.com/awslabs/data-transfer-hub/blob/main/docs/tutorial-directconnect-isolated.md]. DTH worker nodes running on EC2 transfer data from bucket in one AWS account to bucket in another AWS account. To access bucket in the account where DTH is deployed, DTH worker nodes use S3 Gateway Endpoint To access bucket in another account, DTH worker nodes use S3 Private Link by S3 Interface Endpoint","title":"Transfer S3 object via Direct Connect"},{"location":"user-guide/tutorial-directconnect/#use-dth-to-transfer-data-via-dx-in-a-non-isolated-network","text":"In this scenario, DTH is deployed in the destination side and within a VPC with public access (has Internet Gateway or NAT), and the source bucket is in the isolated network. Note As DTH deployment VPC has public internet access (IGW or NAT), EC2 worker/finder nodes can access other AWS services used by DTH such as secret managers and download related assets (such as CloudWatch agent, DTH CLI) from internet without any changes. From the Create Transfer Task page, select Create New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source Amazon S3 Compatible Storage . Enter endpoint url , which must be the interface endpoint url, such as https://bucket.vpce-076205013d3a9a2ca-us23z2ze.s3.ap-east-1.vpce.amazonaws.com . You can find the specific url in VPC Endpoint Console DNS names part. Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . Provide destination settings for the S3 buckets. From Engine settings , verify the values and modify them if necessary. For incremental data transfer, we recommend to set the minimum capacity to at least 1. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . For Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. If you want to skip the data comparison process and transfer all files, select No . If you only want to synchronize files with differences, select Yes . In Alarm Email , provide an email address. Choose Next and review your task parameter details. Choose Create Task .","title":"Use DTH to transfer data via DX in a non-isolated network "},{"location":"user-guide/tutorial-directconnect/#use-dth-to-transfer-data-via-dx-in-an-isolated-network","text":"In this scenario, DTH is deployed in the destination side and within a VPC without public access (isolated VPC), and the source bucket is also in an isolated network. For details, refer to the [tutorial][https://github.com/awslabs/data-transfer-hub/blob/main/docs/tutorial-directconnect-isolated.md]. DTH worker nodes running on EC2 transfer data from bucket in one AWS account to bucket in another AWS account. To access bucket in the account where DTH is deployed, DTH worker nodes use S3 Gateway Endpoint To access bucket in another account, DTH worker nodes use S3 Private Link by S3 Interface Endpoint","title":"Use DTH to transfer data via DX in an isolated network "},{"location":"user-guide/tutorial-ecr/","text":"You can use the web console to create an Amazon ECR transfer task. For more information about how to launch the web console, see deployment . From the Create Transfer Task page, select Start a New Task , and then select Next . From the Engine options page, under engine, select Amazon ECR , and then choose Next Step . You can also copy image from Docker Hub\uff0cGCR.io\uff0cQuay.io, and so on by choosing Public Container Registry . Specify the transfer task details. In Source Type , select the container warehouse type. In Source settings , enter Source Region and Amazon Web Services Account ID . To create credential information, select Secrets Manager to jump to the AWS Secrets Manager console in the current region. From the left menu, select Secrets , then choose Store a new secret and select the other type of secrets key type. Fill in the access_key_id and secret_access_key information in the Plaintext input box according to the displayed format. For more information, refer to IAM features in the IAM User Guide . Choose Next . (Optional) Enter the key name and description. Choose Next . In the configuration of automatic rotation, select Disable automatic rotation. Choose Next . Keep the default value and choose Save to complete the creation of the key. Navigate back to the Data Transfer Hub task creation interface and refresh the interface. Your new secret is displayed in the drop-down list. Select the certificate (Secret). Note If the source is in the same account with Data Transfer Hub deployment, you need to create/provide credential info for the destination. Otherwise, no credential information is needed. Enter an email address in Alarm Email . Choose Next and review your task parameter details. Choose Create Task . After the task is created successfully, it will appear on the Tasks page.","title":"Create Amazon ECR transfer task"},{"location":"user-guide/tutorial-oss/","text":"This tutorial describes how to transfer Objects from Alibaba Cloud OSS to Amazon S3 . Prerequisite You have already deployed the Data Transfer Hub in Oregon (us-west-2) region. For more information, see deployment . Step 1: Configure credentials for OSS Open the Secrets Manager console. Choose Secrets in the left navigation bar. Click Store a new secret button. Select Other type of secrets . Enter the credentials of Alibaba Cloud as text in Plaintext . The credentials are in the format of: { \"access_key_id\": \"<Your Access Key ID>\", \"secret_access_key\": \"<Your Access Key Secret>\" } Click Next . Enter Secret name . For example, dth-oss-credentials . Click Next . Select Disable automatic rotation . Click Store . Step 2: Create an OSS transfer task From the Create Transfer Task page, select Start a New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source Aliyun OSS . Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . Provide destination settings for the S3 buckets. From Engine settings , verify the values and modify them if necessary. For incremental data transfer, set the minimum capacity to at least 1. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . If you need to achieve real-time incremental data synchronization, please refer to the event config guide . For Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. -If you want to skip the data comparison process and transfer all files, select No . -If you only want to synchronize files with differences, select Yes . Enter an email address in Alarm Email . Choose Next and review your task parameter details. Choose Create Task . After the task is created successfully, it will appear on the Tasks page. Transfer task details and status Select the Task ID to go to the task Details page, and then choose CloudWatch Dashboard to monitor the task status. How to achieve real-time data transfer by OSS event trigger If you want to achieve real-time data transfer from Alibaba Cloud OSS to Amazon S3, follow this section to enable OSS event trigger. After you created the task, go to SQS console and record the Queue URL and Queue arn that will be used later. Prepare your AWS account's Access Key/Secret Key Sign in to the IAM console . In the navigation pane, choose Policies , then choose Create poliy . Select the JSON tab, and enter the following information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"sqs:SendMessage\" ], \"Resource\" : \"arn:aws:sqs:us-west-2:xxxxxxxxxxx:DTHS3Stack-S3TransferQueue-1TSF4ESFQEFKJ\" } ] } Note Replace your queue ARN in the JSON. Complete the workflow to create the policy. In the navigation pane, choose Users , then choose Add users . Attach the policy you created previously to the user. Save the ACCESS_KEY/SECRET_KEY which will be used later. Prepare the event-sender function for Alibaba Cloud Open the terminal and enter the following command. You can use docker or Linux machine. mkdir tmp cd tmp pip3 install -t . boto3 Create an index.py in the same folder, and enter the code below. import json import logging import os import boto3 def handler ( event , context ): logger = logging . getLogger () logger . setLevel ( 'INFO' ) evt = json . loads ( event ) if 'events' in evt and len ( evt [ 'events' ]) == 1 : evt = evt [ 'events' ][ 0 ] logger . info ( 'Got event {} ' . format ( evt [ 'eventName' ])) obj = evt [ 'oss' ][ 'object' ] # logger.info(obj) ak = os . environ [ 'ACCESS_KEY' ] sk = os . environ [ 'SECRET_KEY' ] queue_url = os . environ [ 'QUEUE_URL' ] region_name = os . environ [ 'REGION_NAME' ] # minimum info of a message obj_msg = { 'key' : obj [ 'key' ], 'size' : obj [ 'size' ] } # start sending the msg sqs = boto3 . client ( 'sqs' , region_name = region_name , aws_access_key_id = ak , aws_secret_access_key = sk ) try : sqs . send_message ( QueueUrl = queue_url , MessageBody = json . dumps ( obj_msg ) ) except Exception as e : logger . error ( 'Unable to send the message to Amazon SQS, Exception:' , e ) else : logger . warning ( 'Unknown Message ' + evt ) return 'Done' Zip the code (including boto3). zip -r code.zip * Create a function in Alibaba Cloud Use your Alibaba Cloud account to log in to Function Compute , and select Task . Choose Create Function . Choose Python3.x as the Runtime Environments . In Code Upload Method , choose Upload ZIP . Upload the code.zip created in the previous step to create the function. Select Create . Configure the function's environment variables Choose the Configurations . Select Modify in the Environment Variables . Enter the config JSON in the Environment Variables . Here you need to use your own ACCESS_KEY , SECRET_KEY and QUEUE_URL . { \"ACCESS_KEY\" : \"XXX\" , \"QUEUE_URL\" : \"https://sqs.us-west-2.amazonaws.com/xxxx/DTHS3Stack-S3TransferQueue-xxxx\" , \"REGION_NAME\" : \"us-west-2\" , \"SECRET_KEY\" : \"XXXXX\" } 4. Click OK . Create the trigger Navigate to the Create Trigger in Triggers tab to create the trigger for the function. Choose OSS as the Trigger Type , choose the bucket name. For Trigger Event , choose: oss:ObjectCreated:PutObject oss:ObjectCreated:PostObject oss:ObjectCreated:CopyObject oss:ObjectCreated:CompleteMultipartUpload oss:ObjectCreated:AppendObject Select OK .","title":"Transfer S3 object from Alibaba Cloud OSS"},{"location":"user-guide/tutorial-oss/#prerequisite","text":"You have already deployed the Data Transfer Hub in Oregon (us-west-2) region. For more information, see deployment .","title":"Prerequisite"},{"location":"user-guide/tutorial-oss/#step-1-configure-credentials-for-oss","text":"Open the Secrets Manager console. Choose Secrets in the left navigation bar. Click Store a new secret button. Select Other type of secrets . Enter the credentials of Alibaba Cloud as text in Plaintext . The credentials are in the format of: { \"access_key_id\": \"<Your Access Key ID>\", \"secret_access_key\": \"<Your Access Key Secret>\" } Click Next . Enter Secret name . For example, dth-oss-credentials . Click Next . Select Disable automatic rotation . Click Store .","title":"Step 1: Configure credentials for OSS"},{"location":"user-guide/tutorial-oss/#step-2-create-an-oss-transfer-task","text":"From the Create Transfer Task page, select Start a New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source Aliyun OSS . Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . Provide destination settings for the S3 buckets. From Engine settings , verify the values and modify them if necessary. For incremental data transfer, set the minimum capacity to at least 1. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . If you need to achieve real-time incremental data synchronization, please refer to the event config guide . For Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. -If you want to skip the data comparison process and transfer all files, select No . -If you only want to synchronize files with differences, select Yes . Enter an email address in Alarm Email . Choose Next and review your task parameter details. Choose Create Task . After the task is created successfully, it will appear on the Tasks page. Transfer task details and status Select the Task ID to go to the task Details page, and then choose CloudWatch Dashboard to monitor the task status.","title":"Step 2: Create an OSS transfer task"},{"location":"user-guide/tutorial-oss/#how-to-achieve-real-time-data-transfer-by-oss-event-trigger","text":"If you want to achieve real-time data transfer from Alibaba Cloud OSS to Amazon S3, follow this section to enable OSS event trigger. After you created the task, go to SQS console and record the Queue URL and Queue arn that will be used later. Prepare your AWS account's Access Key/Secret Key Sign in to the IAM console . In the navigation pane, choose Policies , then choose Create poliy . Select the JSON tab, and enter the following information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"sqs:SendMessage\" ], \"Resource\" : \"arn:aws:sqs:us-west-2:xxxxxxxxxxx:DTHS3Stack-S3TransferQueue-1TSF4ESFQEFKJ\" } ] } Note Replace your queue ARN in the JSON. Complete the workflow to create the policy. In the navigation pane, choose Users , then choose Add users . Attach the policy you created previously to the user. Save the ACCESS_KEY/SECRET_KEY which will be used later. Prepare the event-sender function for Alibaba Cloud Open the terminal and enter the following command. You can use docker or Linux machine. mkdir tmp cd tmp pip3 install -t . boto3 Create an index.py in the same folder, and enter the code below. import json import logging import os import boto3 def handler ( event , context ): logger = logging . getLogger () logger . setLevel ( 'INFO' ) evt = json . loads ( event ) if 'events' in evt and len ( evt [ 'events' ]) == 1 : evt = evt [ 'events' ][ 0 ] logger . info ( 'Got event {} ' . format ( evt [ 'eventName' ])) obj = evt [ 'oss' ][ 'object' ] # logger.info(obj) ak = os . environ [ 'ACCESS_KEY' ] sk = os . environ [ 'SECRET_KEY' ] queue_url = os . environ [ 'QUEUE_URL' ] region_name = os . environ [ 'REGION_NAME' ] # minimum info of a message obj_msg = { 'key' : obj [ 'key' ], 'size' : obj [ 'size' ] } # start sending the msg sqs = boto3 . client ( 'sqs' , region_name = region_name , aws_access_key_id = ak , aws_secret_access_key = sk ) try : sqs . send_message ( QueueUrl = queue_url , MessageBody = json . dumps ( obj_msg ) ) except Exception as e : logger . error ( 'Unable to send the message to Amazon SQS, Exception:' , e ) else : logger . warning ( 'Unknown Message ' + evt ) return 'Done' Zip the code (including boto3). zip -r code.zip * Create a function in Alibaba Cloud Use your Alibaba Cloud account to log in to Function Compute , and select Task . Choose Create Function . Choose Python3.x as the Runtime Environments . In Code Upload Method , choose Upload ZIP . Upload the code.zip created in the previous step to create the function. Select Create . Configure the function's environment variables Choose the Configurations . Select Modify in the Environment Variables . Enter the config JSON in the Environment Variables . Here you need to use your own ACCESS_KEY , SECRET_KEY and QUEUE_URL . { \"ACCESS_KEY\" : \"XXX\" , \"QUEUE_URL\" : \"https://sqs.us-west-2.amazonaws.com/xxxx/DTHS3Stack-S3TransferQueue-xxxx\" , \"REGION_NAME\" : \"us-west-2\" , \"SECRET_KEY\" : \"XXXXX\" } 4. Click OK .","title":"How to achieve real-time data transfer by OSS event trigger "},{"location":"user-guide/tutorial-oss/#create-the-trigger","text":"Navigate to the Create Trigger in Triggers tab to create the trigger for the function. Choose OSS as the Trigger Type , choose the bucket name. For Trigger Event , choose: oss:ObjectCreated:PutObject oss:ObjectCreated:PostObject oss:ObjectCreated:CopyObject oss:ObjectCreated:CompleteMultipartUpload oss:ObjectCreated:AppendObject Select OK .","title":"Create the trigger"},{"location":"user-guide/tutorial-s3/","text":"You can use the web console to create an Amazon S3 transfer task. For more information about how to launch the web console, see deployment . Note Data Transfer Hub also supports using AWS CLI to create an Amazon S3 transfer task. For details, refer to this tutorial . From the Create Transfer Task page, select Start a New Task , and then select Next . From the Engine options page, under engine, select Amazon S3 , and then choose Next Step . Specify the transfer task details. Under Source Type , select the data source, for example, Amazon S3 . Enter bucket name and choose to sync Full Bucket or Objects with a specific prefix or Objects with different prefixes . If the data source bucket is also in the account deployed by the solution, please select Yes . If you need to achieve real-time incremental data synchronization, please configure whether to enable S3 event notification. Note that this option can only be configured when the program and your data source are deployed in the same area of the same account. If you do not enable S3 event notification, the program will periodically synchronize incremental data according to the scheduling frequency you configure in the future. If the source bucket is not in the same account where Data Transfer Hub was deployed, select No , then specify the credentials for the source bucket. If you choose to synchronize objects with multiple prefixes, please transfer the prefix list file separated by rows to the root directory of the data source bucket, and then fill in the name of the file. For details, please refer to Multi-Prefix List Configuration Tutorial \u3002 To create credential information, select Secrets Manager to jump to the AWS Secrets Manager console in the current region. From the left menu, select Secrets , then choose Store a new secret and select the other type of secrets key type. Fill in the access_key_id and secret_access_key information in the Plaintext input box according to the following format. For more information, refer to IAM features in the IAM User Guide . Choose Next . { \"access_key_id\" : \"<Your Access Key ID>\" , \"secret_access_key\" : \"<Your Access Key Secret>\" } (Optional) Enter the key name and description. Choose Next . In the configuration of automatic rotation, select Disable automatic rotation. Choose Next . Keep the default value and choose Save to complete the creation of the key. Navigate back to the Data Transfer Hub task creation interface and refresh the interface. Your new secret is displayed in the drop-down list. Select the certificate (Secret). Provide destination settings for the S3 buckets. Note If the source S3 bucket is in the same account where Data Transfer Hub was deployed, then in destination settings, you must create or provide credential information for the S3 destination bucket. Otherwise, no credential information is needed. Use the following steps to update the destination settings. From Engine settings , verify the values and modify them if necessary. We recommend to have the minimum capacity set to at least 1 if for incremental data transfer. At Task Scheduling Settings , select your task scheduling configuration. If you want to configure the timed task at a fixed frequency to compare the data difference on both sides of the time, select Fixed Rate . If you want to configure a scheduled task through Cron Expression to achieve a scheduled comparison of data differences on both sides, select Cron Expression . If you only want to perform the data synchronization task once, select One Time Transfer . From Advanced Options , keep the default values. At Need Data Comparison before Transfer , select your task configuration. If you want to skip the data comparison process and transfer all files, please select No . If you only want to synchronize files with differences, please select Yes . Enter an email address in Alarm Email . Choose Next and review your task parameter details. Choose Create Task . After the task is created successfully, it will appear on the Tasks page. How to transfer S3 object from KMS encrypted Amazon S3 By default, Data Transfer Hub supports data source bucket using SSE-S3 and SSE-KMS. If your source bucket enabled SSE-CMK , you need to create an IAM Policy and attach it to DTH worker and finder node. You can go to Amazon IAM Roles Console and search for <StackName>-FinderStackFinderRole<random suffix> and <StackName>-EC2WorkerStackWorkerAsgRole<random suffix> . Pay attention to the following: Change the Resource in KMS part to your own KMS key's Amazon Resource Name (ARN). For S3 buckets in AWS China Regions, make sure to use arn:aws-cn:kms::: instead of arn:aws:kms::: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\", \"kms:Encrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": [ \"arn:aws:kms:us-west-2:123456789012:key/f5cd8cb7-476c-4322-ac9b-0c94a687700d <Please replace this with your own KMS key arn>\" ] } ] }","title":"Create Amazon S3 transfer task"},{"location":"user-guide/tutorial-s3/#how-to-transfer-s3-object-from-kms-encrypted-amazon-s3","text":"By default, Data Transfer Hub supports data source bucket using SSE-S3 and SSE-KMS. If your source bucket enabled SSE-CMK , you need to create an IAM Policy and attach it to DTH worker and finder node. You can go to Amazon IAM Roles Console and search for <StackName>-FinderStackFinderRole<random suffix> and <StackName>-EC2WorkerStackWorkerAsgRole<random suffix> . Pay attention to the following: Change the Resource in KMS part to your own KMS key's Amazon Resource Name (ARN). For S3 buckets in AWS China Regions, make sure to use arn:aws-cn:kms::: instead of arn:aws:kms::: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\", \"kms:Encrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": [ \"arn:aws:kms:us-west-2:123456789012:key/f5cd8cb7-476c-4322-ac9b-0c94a687700d <Please replace this with your own KMS key arn>\" ] } ] }","title":"How to transfer S3 object from KMS encrypted Amazon S3"}]}